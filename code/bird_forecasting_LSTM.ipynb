{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e23d80cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model, save_model\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9176cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_model(x, y_train, test):\n",
    "    x_train = np.reshape( x, (x.shape[0], x.shape[1], 1) )\n",
    "    model = Sequential()\n",
    "    model.add( LSTM( 100, input_shape=(x_train.shape[1], x_train.shape[2]), return_sequences=True) )\n",
    "    model.add( LSTM( 20, return_sequences=False ) )\n",
    "    model.add( Dropout( 0.2 ) )\n",
    "    model.add( Dense( 1 ) )\n",
    "    model.add( Activation( 'linear' ) )\n",
    "    model.compile( loss=\"mse\", optimizer=\"rmsprop\" )\n",
    "    model.fit( x_train, y_train, epochs=200, batch_size=1)\n",
    "    test = np.reshape( test, (test.shape[0], test.shape[1], 1) )\n",
    "    res = model.predict( test )\n",
    "    return res\n",
    "\n",
    "def drop_altitude(data):\n",
    "    df = []\n",
    "    for i in tqdm(np.arange(0,len(data), 30)):\n",
    "        df.append(data[i:i+20].sum())\n",
    "    print(df)\n",
    "    #return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961fb99e",
   "metadata": {},
   "source": [
    "### intensity lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b50c5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecasting = pd.read_csv('../dataset/radar/features/intensity_lags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f1bf2b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "453/453 [==============================] - 31s 5ms/step - loss: 24208283.9945\n",
      "Epoch 2/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 14144372.4414\n",
      "Epoch 3/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 9440946.1225\n",
      "Epoch 4/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 5484228.9132\n",
      "Epoch 5/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 16512157.1509\n",
      "Epoch 6/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 11580777.9695\n",
      "Epoch 7/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 18864768.8734\n",
      "Epoch 8/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 20833546.3675\n",
      "Epoch 9/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 16136129.7607\n",
      "Epoch 10/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 22947583.5986\n",
      "Epoch 11/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 26600931.1960\n",
      "Epoch 12/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 15275778.2581\n",
      "Epoch 13/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 13934281.4457\n",
      "Epoch 14/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 18163377.7264\n",
      "Epoch 15/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 11967340.1046\n",
      "Epoch 16/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 15994380.4892\n",
      "Epoch 17/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 14336289.4695\n",
      "Epoch 18/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 19596153.7244\n",
      "Epoch 19/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 19306401.3315\n",
      "Epoch 20/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 18636527.0719\n",
      "Epoch 21/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 7867221.4924\n",
      "Epoch 22/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 15056648.4686\n",
      "Epoch 23/200\n",
      "453/453 [==============================] - 2s 6ms/step - loss: 8649649.0957\n",
      "Epoch 24/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 12854418.3069\n",
      "Epoch 25/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 19894652.8786\n",
      "Epoch 26/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 18426796.4959\n",
      "Epoch 27/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 11124661.3023\n",
      "Epoch 28/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 28109715.6464\n",
      "Epoch 29/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 18232276.4936\n",
      "Epoch 30/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 10151747.6476\n",
      "Epoch 31/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 9563217.3273\n",
      "Epoch 32/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 12121785.4006\n",
      "Epoch 33/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 18872233.0209\n",
      "Epoch 34/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 15884424.5087\n",
      "Epoch 35/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 6590153.0363\n",
      "Epoch 36/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 7317246.3635\n",
      "Epoch 37/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 12067853.7810\n",
      "Epoch 38/200\n",
      "453/453 [==============================] - 2s 6ms/step - loss: 15783916.3965\n",
      "Epoch 39/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 12851595.1198\n",
      "Epoch 40/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 12398367.7537\n",
      "Epoch 41/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 20581708.7685\n",
      "Epoch 42/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 11512297.3562\n",
      "Epoch 43/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 6470761.0498\n",
      "Epoch 44/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 16867901.5508\n",
      "Epoch 45/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 16137999.3660\n",
      "Epoch 46/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 21405880.2428\n",
      "Epoch 47/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 13738276.6969\n",
      "Epoch 48/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 25156025.6921\n",
      "Epoch 49/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 24423882.7885\n",
      "Epoch 50/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 10177687.3737\n",
      "Epoch 51/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 15532721.5068\n",
      "Epoch 52/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 11556685.0630\n",
      "Epoch 53/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 14532220.4565\n",
      "Epoch 54/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 21049337.1799\n",
      "Epoch 55/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 19328995.2818\n",
      "Epoch 56/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 15254271.6534\n",
      "Epoch 57/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 18513429.9262\n",
      "Epoch 58/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 13056847.7451\n",
      "Epoch 59/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 28567590.4708\n",
      "Epoch 60/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 8758601.5889\n",
      "Epoch 61/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 24688155.6624\n",
      "Epoch 62/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 14628342.8091\n",
      "Epoch 63/200\n",
      "453/453 [==============================] - 3s 7ms/step - loss: 17410090.1423\n",
      "Epoch 64/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 14215682.8627\n",
      "Epoch 65/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 10065480.8659\n",
      "Epoch 66/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 12724839.4401\n",
      "Epoch 67/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 17098098.3376\n",
      "Epoch 68/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 40494525.6498\n",
      "Epoch 69/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 7203166.2834\n",
      "Epoch 70/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 13655172.7587\n",
      "Epoch 71/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 21945234.7285\n",
      "Epoch 72/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 15156965.3575\n",
      "Epoch 73/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 14406723.4119\n",
      "Epoch 74/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 10841443.7065\n",
      "Epoch 75/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 9816152.2368\n",
      "Epoch 76/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 15658711.8664\n",
      "Epoch 77/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 8232971.9459\n",
      "Epoch 78/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 12121459.6810\n",
      "Epoch 79/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 17757848.9031\n",
      "Epoch 80/200\n",
      "453/453 [==============================] - 4s 10ms/step - loss: 14220396.8876\n",
      "Epoch 81/200\n",
      "453/453 [==============================] - 3s 7ms/step - loss: 14112242.0340\n",
      "Epoch 82/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 14763317.6124\n",
      "Epoch 83/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 19109380.0701\n",
      "Epoch 84/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 20619380.7859\n",
      "Epoch 85/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 19009600.8943\n",
      "Epoch 86/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 24816279.0374\n",
      "Epoch 87/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 14912766.5580\n",
      "Epoch 88/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 13069573.2521\n",
      "Epoch 89/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 16208895.2305\n",
      "Epoch 90/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 7898673.2402\n",
      "Epoch 91/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 19352679.7110\n",
      "Epoch 92/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 2s 6ms/step - loss: 9406074.0393\n",
      "Epoch 93/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 23265691.8221\n",
      "Epoch 94/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 10198157.7415\n",
      "Epoch 95/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 14828785.9109\n",
      "Epoch 96/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 7382124.7521\n",
      "Epoch 97/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 13323662.2130\n",
      "Epoch 98/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 16910833.1354\n",
      "Epoch 99/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 16603126.5158\n",
      "Epoch 100/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 11942429.5633\n",
      "Epoch 101/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 6698303.1754\n",
      "Epoch 102/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 18961149.7500\n",
      "Epoch 103/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 24226144.9446\n",
      "Epoch 104/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 14880090.2657\n",
      "Epoch 105/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 28451422.9349\n",
      "Epoch 106/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 10359708.1011\n",
      "Epoch 107/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 8592611.5499\n",
      "Epoch 108/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 14871118.8722\n",
      "Epoch 109/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 16904054.3323\n",
      "Epoch 110/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 13080222.2687\n",
      "Epoch 111/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 8159839.3032\n",
      "Epoch 112/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 18846833.9086\n",
      "Epoch 113/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 11852634.3598\n",
      "Epoch 114/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 21028872.9202\n",
      "Epoch 115/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 14818339.5255\n",
      "Epoch 116/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 10491791.0612\n",
      "Epoch 117/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 11993691.9631\n",
      "Epoch 118/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 10596643.5004\n",
      "Epoch 119/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 12470904.5594\n",
      "Epoch 120/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 27651294.0842\n",
      "Epoch 121/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 6943492.9690\n",
      "Epoch 122/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 15506499.2426\n",
      "Epoch 123/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 18291647.9590\n",
      "Epoch 124/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 12810657.7249\n",
      "Epoch 125/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 12400064.9762\n",
      "Epoch 126/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 11251406.3919\n",
      "Epoch 127/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 17725547.1270\n",
      "Epoch 128/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 10328543.0572\n",
      "Epoch 129/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 28367915.4967\n",
      "Epoch 130/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 32024104.3025\n",
      "Epoch 131/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 13587337.7499\n",
      "Epoch 132/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 7649486.2226\n",
      "Epoch 133/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 12521329.7555\n",
      "Epoch 134/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 16021070.4795\n",
      "Epoch 135/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 19976963.3641\n",
      "Epoch 136/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 6301357.7196\n",
      "Epoch 137/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 14825969.8099\n",
      "Epoch 138/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 8339596.6458\n",
      "Epoch 139/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 15659475.5562\n",
      "Epoch 140/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 10298820.2157\n",
      "Epoch 141/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 18389903.8586\n",
      "Epoch 142/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 18451903.1360\n",
      "Epoch 143/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 6489765.6767\n",
      "Epoch 144/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 12099017.5739\n",
      "Epoch 145/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 7134990.9452\n",
      "Epoch 146/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 22106202.4956\n",
      "Epoch 147/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 9906394.5821\n",
      "Epoch 148/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 15576228.4177\n",
      "Epoch 149/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 9614835.0284\n",
      "Epoch 150/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 12393213.4262\n",
      "Epoch 151/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 10989342.2115\n",
      "Epoch 152/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 8857064.3103\n",
      "Epoch 153/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 9245507.0036\n",
      "Epoch 154/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 10263227.1083\n",
      "Epoch 155/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 7557070.7671\n",
      "Epoch 156/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 19954012.5780\n",
      "Epoch 157/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 13308731.1058\n",
      "Epoch 158/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 12806151.9124\n",
      "Epoch 159/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 8730534.1639\n",
      "Epoch 160/200\n",
      "453/453 [==============================] - 2s 6ms/step - loss: 21583963.4718\n",
      "Epoch 161/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 12744442.2567\n",
      "Epoch 162/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 18014787.8116\n",
      "Epoch 163/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 13126654.7953\n",
      "Epoch 164/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 15990153.3215\n",
      "Epoch 165/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 9200003.3028\n",
      "Epoch 166/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 15896073.0946\n",
      "Epoch 167/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 11853218.9865\n",
      "Epoch 168/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 7810228.3652\n",
      "Epoch 169/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 16950444.9153\n",
      "Epoch 170/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 8486949.1948\n",
      "Epoch 171/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 16093380.3450\n",
      "Epoch 172/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 13510741.6562\n",
      "Epoch 173/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 8559216.2160\n",
      "Epoch 174/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 10177227.4715\n",
      "Epoch 175/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 10726483.2217\n",
      "Epoch 176/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 13892086.3297\n",
      "Epoch 177/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 8497118.6626\n",
      "Epoch 178/200\n",
      "453/453 [==============================] - 2s 6ms/step - loss: 8800410.0359\n",
      "Epoch 179/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 15316341.9477\n",
      "Epoch 180/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 10751610.8337\n",
      "Epoch 181/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 27040521.2456\n",
      "Epoch 182/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 2s 5ms/step - loss: 13424863.5266\n",
      "Epoch 183/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 12376138.6493\n",
      "Epoch 184/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 10316691.4756\n",
      "Epoch 185/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 10926084.2960\n",
      "Epoch 186/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 10983343.7602\n",
      "Epoch 187/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 10199237.0230\n",
      "Epoch 188/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 14156102.9560\n",
      "Epoch 189/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 16545953.2390\n",
      "Epoch 190/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 9565428.5082\n",
      "Epoch 191/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 16862531.4601\n",
      "Epoch 192/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 12956950.6072\n",
      "Epoch 193/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 12672938.7271\n",
      "Epoch 194/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 13582078.7283\n",
      "Epoch 195/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 7964153.3237\n",
      "Epoch 196/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 16750564.6443\n",
      "Epoch 197/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 22390257.8776\n",
      "Epoch 198/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 8599221.5746\n",
      "Epoch 199/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 21044408.9757\n",
      "Epoch 200/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 16118031.9115\n"
     ]
    }
   ],
   "source": [
    "coef = 0.5\n",
    "X, y = df_forecasting.iloc[:,1:], df_forecasting.iloc[:,0] \n",
    "trainX, testX = X[:int(coef * len(X))].values, X[int(coef * len(X)):].values\n",
    "trainY, testY = y[:int(coef * len(y))].values, y[int(coef * len(y)):].values\n",
    "#trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "#testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "pred = LSTM_model(trainX, trainY, testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aeccadfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844.8714096903909 -0.04419887245810705 0.011539117774139518\n"
     ]
    }
   ],
   "source": [
    "mae = metrics.mean_absolute_error(testY, pred)\n",
    "r2 = metrics.r2_score(testY, pred)\n",
    "var = metrics.explained_variance_score(testY, pred)\n",
    "print(mae, r2, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af648a2",
   "metadata": {},
   "source": [
    "### lags mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "763041a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "414/414 [==============================] - 7s 5ms/step - loss: 20087698.6238\n",
      "Epoch 2/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 14763322.3452\n",
      "Epoch 3/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 19733465.7686\n",
      "Epoch 4/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 11844698.8871\n",
      "Epoch 5/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 16409961.8557\n",
      "Epoch 6/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 19740131.7526\n",
      "Epoch 7/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 11111298.0359\n",
      "Epoch 8/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 12385388.1262\n",
      "Epoch 9/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 11577016.7884\n",
      "Epoch 10/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 14578604.4914\n",
      "Epoch 11/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 13263187.2590\n",
      "Epoch 12/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 6377017.2459\n",
      "Epoch 13/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 12757268.9381\n",
      "Epoch 14/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 10599015.5498\n",
      "Epoch 15/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 7347797.1965\n",
      "Epoch 16/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 16847638.1394\n",
      "Epoch 17/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 8614524.5912\n",
      "Epoch 18/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 11396851.6736\n",
      "Epoch 19/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 17296468.5396\n",
      "Epoch 20/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 9324722.9484\n",
      "Epoch 21/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 10174352.8628\n",
      "Epoch 22/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 11364081.6755\n",
      "Epoch 23/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 21076926.5558\n",
      "Epoch 24/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 13773771.0214\n",
      "Epoch 25/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 7130496.0563\n",
      "Epoch 26/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 13191904.0234\n",
      "Epoch 27/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 20134611.6128\n",
      "Epoch 28/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 21319012.3267\n",
      "Epoch 29/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 21999396.4651\n",
      "Epoch 30/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 9166098.3711\n",
      "Epoch 31/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 8608186.9405\n",
      "Epoch 32/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 9449433.1426\n",
      "Epoch 33/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 10811263.4930\n",
      "Epoch 34/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 21171783.3651\n",
      "Epoch 35/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 9970208.7410\n",
      "Epoch 36/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 11070815.1550\n",
      "Epoch 37/200\n",
      "414/414 [==============================] - 3s 7ms/step - loss: 10865404.5283\n",
      "Epoch 38/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 7781495.9495\n",
      "Epoch 39/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 13681770.8113\n",
      "Epoch 40/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 7936201.6500\n",
      "Epoch 41/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 21682618.0160\n",
      "Epoch 42/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 25418225.0722\n",
      "Epoch 43/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 12563448.9779\n",
      "Epoch 44/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 15613694.1481\n",
      "Epoch 45/200\n",
      "414/414 [==============================] - 3s 6ms/step - loss: 15736666.3389\n",
      "Epoch 46/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 29571956.9349\n",
      "Epoch 47/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 10840438.7744\n",
      "Epoch 48/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 14311381.6480\n",
      "Epoch 49/200\n",
      "414/414 [==============================] - 3s 6ms/step - loss: 17145894.6780\n",
      "Epoch 50/200\n",
      "414/414 [==============================] - 3s 6ms/step - loss: 21054827.3777\n",
      "Epoch 51/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 25110657.2969\n",
      "Epoch 52/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 9169184.7705\n",
      "Epoch 53/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 11621897.4710\n",
      "Epoch 54/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 8097085.1678\n",
      "Epoch 55/200\n",
      "414/414 [==============================] - 3s 6ms/step - loss: 11484544.3303\n",
      "Epoch 56/200\n",
      "414/414 [==============================] - 3s 6ms/step - loss: 10701578.5396\n",
      "Epoch 57/200\n",
      "414/414 [==============================] - 3s 6ms/step - loss: 7133626.1264\n",
      "Epoch 58/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 8049903.3922\n",
      "Epoch 59/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 10380322.0199\n",
      "Epoch 60/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 11410073.8037\n",
      "Epoch 61/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 12745591.5539\n",
      "Epoch 62/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 10667945.3552\n",
      "Epoch 63/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 17743636.6090\n",
      "Epoch 64/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 9689548.3738\n",
      "Epoch 65/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 15446438.0799\n",
      "Epoch 66/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 16058235.6863\n",
      "Epoch 67/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 12593853.2869\n",
      "Epoch 68/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 18965371.2483\n",
      "Epoch 69/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 16531820.0756\n",
      "Epoch 70/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 15406587.6714\n",
      "Epoch 71/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 6252344.9008\n",
      "Epoch 72/200\n",
      "414/414 [==============================] - 3s 6ms/step - loss: 19591311.6999\n",
      "Epoch 73/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 14464285.0588\n",
      "Epoch 74/200\n",
      "414/414 [==============================] - 3s 6ms/step - loss: 12424501.7315\n",
      "Epoch 75/200\n",
      "414/414 [==============================] - 3s 6ms/step - loss: 18272687.3146\n",
      "Epoch 76/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 16838068.1444\n",
      "Epoch 77/200\n",
      "414/414 [==============================] - 3s 6ms/step - loss: 10467948.7393\n",
      "Epoch 78/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 10496550.4808\n",
      "Epoch 79/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 8405206.7854\n",
      "Epoch 80/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 18777259.2600\n",
      "Epoch 81/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 19515890.4361\n",
      "Epoch 82/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 17550119.7733\n",
      "Epoch 83/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 10049256.5251\n",
      "Epoch 84/200\n",
      "414/414 [==============================] - 3s 6ms/step - loss: 4801003.3563\n",
      "Epoch 85/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 14900353.1289\n",
      "Epoch 86/200\n",
      "414/414 [==============================] - 3s 6ms/step - loss: 12759588.0687\n",
      "Epoch 87/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 14374339.0713\n",
      "Epoch 88/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 23711851.7799\n",
      "Epoch 89/200\n",
      "414/414 [==============================] - 3s 6ms/step - loss: 15151713.2996\n",
      "Epoch 90/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 7114738.0251\n",
      "Epoch 91/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 8994386.8778\n",
      "Epoch 92/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414/414 [==============================] - 2s 6ms/step - loss: 17089392.8174\n",
      "Epoch 93/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 17258083.8903\n",
      "Epoch 94/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 15932540.9561\n",
      "Epoch 95/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 16537880.4851\n",
      "Epoch 96/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 10957601.1771\n",
      "Epoch 97/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 10597531.0002: 0s - l\n",
      "Epoch 98/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 13954667.8455\n",
      "Epoch 99/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 9984614.5515\n",
      "Epoch 100/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 8824033.7434\n",
      "Epoch 101/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 18797959.3841\n",
      "Epoch 102/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 7306337.1023\n",
      "Epoch 103/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 12740737.4223\n",
      "Epoch 104/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 23865744.4524\n",
      "Epoch 105/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 13063131.7769\n",
      "Epoch 106/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 9179838.6115\n",
      "Epoch 107/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 8674669.6460\n",
      "Epoch 108/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 19221348.3110\n",
      "Epoch 109/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 10741818.2834\n",
      "Epoch 110/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 8259685.8864\n",
      "Epoch 111/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 6764060.0067\n",
      "Epoch 112/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 11391740.3196\n",
      "Epoch 113/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 14826008.8184\n",
      "Epoch 114/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 17497707.2105\n",
      "Epoch 115/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 12726324.5574\n",
      "Epoch 116/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 15686690.1993\n",
      "Epoch 117/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 14506495.4534\n",
      "Epoch 118/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 5224741.6354\n",
      "Epoch 119/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 11824889.4302\n",
      "Epoch 120/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 12372817.2493\n",
      "Epoch 121/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 14477035.1725\n",
      "Epoch 122/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 13199179.3446\n",
      "Epoch 123/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 10091739.8803\n",
      "Epoch 124/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 9640354.9319\n",
      "Epoch 125/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 18479173.0319: 0s - loss: 1886125\n",
      "Epoch 126/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 24151933.4562\n",
      "Epoch 127/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 17568979.9301\n",
      "Epoch 128/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 9085088.6803\n",
      "Epoch 129/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 18040540.5589\n",
      "Epoch 130/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 19867677.7587\n",
      "Epoch 131/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 9432948.9995\n",
      "Epoch 132/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 6240551.0567\n",
      "Epoch 133/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 14142974.3167\n",
      "Epoch 134/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 11506311.9202\n",
      "Epoch 135/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 6148606.6409\n",
      "Epoch 136/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 18752537.3664\n",
      "Epoch 137/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 17545120.8678\n",
      "Epoch 138/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 14650154.4626\n",
      "Epoch 139/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 9023218.9557\n",
      "Epoch 140/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 8507853.2795\n",
      "Epoch 141/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 12055728.8717\n",
      "Epoch 142/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 13390221.2017\n",
      "Epoch 143/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 8950547.1708\n",
      "Epoch 144/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 24489483.6233\n",
      "Epoch 145/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 12172462.9832\n",
      "Epoch 146/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 18579016.0834\n",
      "Epoch 147/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 7561722.0693\n",
      "Epoch 148/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 18059165.8368\n",
      "Epoch 149/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 13085182.3093\n",
      "Epoch 150/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 12496561.7070\n",
      "Epoch 151/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 8631574.0731\n",
      "Epoch 152/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 10095631.4065\n",
      "Epoch 153/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 13567988.2462\n",
      "Epoch 154/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 13641596.2132\n",
      "Epoch 155/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 8381417.4773\n",
      "Epoch 156/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 21992393.3223\n",
      "Epoch 157/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 12098048.8196\n",
      "Epoch 158/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 8584479.7182\n",
      "Epoch 159/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 9893657.9173\n",
      "Epoch 160/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 12941732.2769\n",
      "Epoch 161/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 11712670.7451\n",
      "Epoch 162/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 17637091.4826\n",
      "Epoch 163/200\n",
      "414/414 [==============================] - 2s 4ms/step - loss: 10608134.7014\n",
      "Epoch 164/200\n",
      "414/414 [==============================] - 2s 4ms/step - loss: 6765826.3871\n",
      "Epoch 165/200\n",
      "414/414 [==============================] - 2s 4ms/step - loss: 9198844.0241\n",
      "Epoch 166/200\n",
      "414/414 [==============================] - 2s 4ms/step - loss: 22329625.3744\n",
      "Epoch 167/200\n",
      "414/414 [==============================] - 2s 4ms/step - loss: 6948718.3309\n",
      "Epoch 168/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 12027763.5849\n",
      "Epoch 169/200\n",
      "414/414 [==============================] - 2s 4ms/step - loss: 18397959.5963\n",
      "Epoch 170/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 6309216.1750\n",
      "Epoch 171/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 12499650.4886\n",
      "Epoch 172/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 5890349.3741\n",
      "Epoch 173/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 11362030.9651\n",
      "Epoch 174/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 12521801.6104\n",
      "Epoch 175/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 10299467.3813\n",
      "Epoch 176/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 22324694.2530\n",
      "Epoch 177/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 14234931.2393\n",
      "Epoch 178/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 10431065.2790\n",
      "Epoch 179/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 17709053.5673\n",
      "Epoch 180/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 11486697.3251\n",
      "Epoch 181/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 19124544.5253\n",
      "Epoch 182/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414/414 [==============================] - 2s 5ms/step - loss: 13871977.0255\n",
      "Epoch 183/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 7351712.3285\n",
      "Epoch 184/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 8061655.7049\n",
      "Epoch 185/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 20661161.0012\n",
      "Epoch 186/200\n",
      "414/414 [==============================] - 2s 6ms/step - loss: 13184708.0519\n",
      "Epoch 187/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 13739782.9328\n",
      "Epoch 188/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 9441855.5195\n",
      "Epoch 189/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 5992031.2016\n",
      "Epoch 190/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 14424518.7173\n",
      "Epoch 191/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 9546989.1066\n",
      "Epoch 192/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 11716289.4425\n",
      "Epoch 193/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 16567629.1468\n",
      "Epoch 194/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 21504111.6232\n",
      "Epoch 195/200\n",
      "414/414 [==============================] - 2s 5ms/step - loss: 12889169.4253\n",
      "Epoch 196/200\n",
      "414/414 [==============================] - 2s 4ms/step - loss: 12418700.2877\n",
      "Epoch 197/200\n",
      "414/414 [==============================] - 2s 4ms/step - loss: 16016210.8281\n",
      "Epoch 198/200\n",
      "414/414 [==============================] - 2s 4ms/step - loss: 16578731.0060\n",
      "Epoch 199/200\n",
      "414/414 [==============================] - 2s 4ms/step - loss: 11376937.0757\n",
      "Epoch 200/200\n",
      "414/414 [==============================] - 2s 4ms/step - loss: 13185766.4357\n",
      "863.1433117297181 -0.048260016912953674 0.013690132481536055\n"
     ]
    }
   ],
   "source": [
    "df_forecasting = pd.read_csv('../dataset/radar/features/lags_mean.csv')\n",
    "coef = 0.5\n",
    "X, y = df_forecasting.iloc[:,1:], df_forecasting.iloc[:,0] \n",
    "trainX, testX = X[:int(coef * len(X))].values, X[int(coef * len(X)):].values\n",
    "trainY, testY = y[:int(coef * len(y))].values, y[int(coef * len(y)):].values\n",
    "#trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "#testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "pred = LSTM_model(trainX, trainY, testX)\n",
    "mae = metrics.mean_absolute_error(testY, pred)\n",
    "r2 = metrics.r2_score(testY, pred)\n",
    "var = metrics.explained_variance_score(testY, pred)\n",
    "print(mae, r2, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c04c5f",
   "metadata": {},
   "source": [
    "### weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4edcf3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "460/460 [==============================] - 9s 7ms/step - loss: 25050470.2719\n",
      "Epoch 2/200\n",
      "460/460 [==============================] - 3s 7ms/step - loss: 9141159.9992\n",
      "Epoch 3/200\n",
      "460/460 [==============================] - 3s 7ms/step - loss: 16948976.1019\n",
      "Epoch 4/200\n",
      "460/460 [==============================] - 3s 7ms/step - loss: 16547131.9740\n",
      "Epoch 5/200\n",
      "460/460 [==============================] - 3s 7ms/step - loss: 10695496.1177\n",
      "Epoch 6/200\n",
      "460/460 [==============================] - 3s 7ms/step - loss: 21636798.0030\n",
      "Epoch 7/200\n",
      "460/460 [==============================] - 3s 7ms/step - loss: 29268594.2643\n",
      "Epoch 8/200\n",
      "460/460 [==============================] - 3s 7ms/step - loss: 13045998.7522\n",
      "Epoch 9/200\n",
      "460/460 [==============================] - 3s 7ms/step - loss: 7696508.1767\n",
      "Epoch 10/200\n",
      "460/460 [==============================] - 3s 7ms/step - loss: 9012265.7292\n",
      "Epoch 11/200\n",
      "460/460 [==============================] - 3s 7ms/step - loss: 14280200.8663\n",
      "Epoch 12/200\n",
      "460/460 [==============================] - 3s 7ms/step - loss: 10983966.5108\n",
      "Epoch 13/200\n",
      "460/460 [==============================] - 3s 7ms/step - loss: 12190123.2060\n",
      "Epoch 14/200\n",
      "460/460 [==============================] - 3s 7ms/step - loss: 10069666.9045\n",
      "Epoch 15/200\n",
      "460/460 [==============================] - 3s 7ms/step - loss: 11275476.2738\n",
      "Epoch 16/200\n",
      "460/460 [==============================] - 4s 8ms/step - loss: 13034139.1268\n",
      "Epoch 17/200\n",
      "460/460 [==============================] - 4s 8ms/step - loss: 5261422.3244\n",
      "Epoch 18/200\n",
      "460/460 [==============================] - 4s 10ms/step - loss: 11594713.5608\n",
      "Epoch 19/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 15185048.5158\n",
      "Epoch 20/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 17252324.1978\n",
      "Epoch 21/200\n",
      "460/460 [==============================] - 3s 7ms/step - loss: 8514356.6222\n",
      "Epoch 22/200\n",
      "460/460 [==============================] - 3s 7ms/step - loss: 33001744.1796\n",
      "Epoch 23/200\n",
      "460/460 [==============================] - 3s 7ms/step - loss: 14558539.6783\n",
      "Epoch 24/200\n",
      "460/460 [==============================] - 4s 8ms/step - loss: 15045544.7403\n",
      "Epoch 25/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 18559795.0493\n",
      "Epoch 26/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 19020959.5651\n",
      "Epoch 27/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 15637036.6154\n",
      "Epoch 28/200\n",
      "460/460 [==============================] - 5s 10ms/step - loss: 13091990.7606\n",
      "Epoch 29/200\n",
      "460/460 [==============================] - 5s 11ms/step - loss: 26224544.9740\n",
      "Epoch 30/200\n",
      "460/460 [==============================] - 5s 10ms/step - loss: 6045109.7375\n",
      "Epoch 31/200\n",
      "460/460 [==============================] - 4s 10ms/step - loss: 17658995.4477\n",
      "Epoch 32/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 13720058.1068\n",
      "Epoch 33/200\n",
      "460/460 [==============================] - 4s 8ms/step - loss: 10607156.1045\n",
      "Epoch 34/200\n",
      "460/460 [==============================] - 4s 8ms/step - loss: 14485095.2095\n",
      "Epoch 35/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 7139091.2612\n",
      "Epoch 36/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 6380090.8182\n",
      "Epoch 37/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 29965903.3600\n",
      "Epoch 38/200\n",
      "460/460 [==============================] - 4s 8ms/step - loss: 14637497.6973\n",
      "Epoch 39/200\n",
      "460/460 [==============================] - 4s 8ms/step - loss: 13039763.2314\n",
      "Epoch 40/200\n",
      "460/460 [==============================] - 4s 8ms/step - loss: 15428280.5471\n",
      "Epoch 41/200\n",
      "460/460 [==============================] - 4s 8ms/step - loss: 18990865.0269\n",
      "Epoch 42/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 13079247.5797\n",
      "Epoch 43/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 19363428.7465\n",
      "Epoch 44/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 12693559.0423\n",
      "Epoch 45/200\n",
      "460/460 [==============================] - 4s 8ms/step - loss: 10434357.6302\n",
      "Epoch 46/200\n",
      "460/460 [==============================] - 4s 8ms/step - loss: 12161755.5114\n",
      "Epoch 47/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 11524587.5607\n",
      "Epoch 48/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 11811637.5315\n",
      "Epoch 49/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 17365737.9722\n",
      "Epoch 50/200\n",
      "460/460 [==============================] - 4s 8ms/step - loss: 12724503.4734\n",
      "Epoch 51/200\n",
      "460/460 [==============================] - 4s 8ms/step - loss: 20253912.7949\n",
      "Epoch 52/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 15945138.1753\n",
      "Epoch 53/200\n",
      "460/460 [==============================] - 4s 8ms/step - loss: 10126446.2088\n",
      "Epoch 54/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 10331183.2555\n",
      "Epoch 55/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 8903928.8915\n",
      "Epoch 56/200\n",
      "460/460 [==============================] - 5s 10ms/step - loss: 13767721.0978\n",
      "Epoch 57/200\n",
      "460/460 [==============================] - 5s 10ms/step - loss: 12313625.9500\n",
      "Epoch 58/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 34163961.7264\n",
      "Epoch 59/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 16362493.1984\n",
      "Epoch 60/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 7865568.7687\n",
      "Epoch 61/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 14361008.2921\n",
      "Epoch 62/200\n",
      "460/460 [==============================] - 4s 8ms/step - loss: 16480164.9429\n",
      "Epoch 63/200\n",
      "460/460 [==============================] - 4s 8ms/step - loss: 16342658.8677\n",
      "Epoch 64/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 16412650.5297\n",
      "Epoch 65/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 10952226.3730\n",
      "Epoch 66/200\n",
      "460/460 [==============================] - 5s 10ms/step - loss: 14049892.0989\n",
      "Epoch 67/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 9817362.4878\n",
      "Epoch 68/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 24452824.4436\n",
      "Epoch 69/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 9791967.0699\n",
      "Epoch 70/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 16993910.4087\n",
      "Epoch 71/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 10947620.6358\n",
      "Epoch 72/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 15442965.3349\n",
      "Epoch 73/200\n",
      "460/460 [==============================] - 4s 8ms/step - loss: 15278240.7666\n",
      "Epoch 74/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 10825863.2111\n",
      "Epoch 75/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 22757408.7837\n",
      "Epoch 76/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 7159320.9116\n",
      "Epoch 77/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 16207091.2171\n",
      "Epoch 78/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 13398763.9402\n",
      "Epoch 79/200\n",
      "460/460 [==============================] - 4s 8ms/step - loss: 10348791.3349\n",
      "Epoch 80/200\n",
      "460/460 [==============================] - 4s 8ms/step - loss: 12411268.6979\n",
      "Epoch 81/200\n",
      "460/460 [==============================] - 4s 8ms/step - loss: 13384109.3865\n",
      "Epoch 82/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 8236448.2960\n",
      "Epoch 83/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 10142418.6739\n",
      "Epoch 84/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 12806843.9620\n",
      "Epoch 85/200\n",
      "460/460 [==============================] - 4s 8ms/step - loss: 15403088.1874\n",
      "Epoch 86/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 15839133.6188\n",
      "Epoch 87/200\n",
      "460/460 [==============================] - 4s 8ms/step - loss: 11100932.5691\n",
      "Epoch 88/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 17263426.2227\n",
      "Epoch 89/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 22777223.0824\n",
      "Epoch 90/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 16695423.6892\n",
      "Epoch 91/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 13604714.7323\n",
      "Epoch 92/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460/460 [==============================] - 4s 9ms/step - loss: 8607436.0421\n",
      "Epoch 93/200\n",
      "460/460 [==============================] - 4s 8ms/step - loss: 12099221.2901\n",
      "Epoch 94/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 14655366.9776\n",
      "Epoch 95/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 17825948.2288\n",
      "Epoch 96/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 12009156.9913\n",
      "Epoch 97/200\n",
      "460/460 [==============================] - 4s 8ms/step - loss: 21206832.1202\n",
      "Epoch 98/200\n",
      "460/460 [==============================] - 4s 8ms/step - loss: 4833348.3549\n",
      "Epoch 99/200\n",
      "460/460 [==============================] - 4s 8ms/step - loss: 9341421.5819\n",
      "Epoch 100/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 12941869.7133\n",
      "Epoch 101/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 16842828.5650\n",
      "Epoch 102/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 13139050.0137\n",
      "Epoch 103/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 22680615.5480\n",
      "Epoch 104/200\n",
      "460/460 [==============================] - 4s 8ms/step - loss: 9007212.8200\n",
      "Epoch 105/200\n",
      "460/460 [==============================] - 4s 8ms/step - loss: 12507856.0115\n",
      "Epoch 106/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 10982222.5832\n",
      "Epoch 107/200\n",
      "460/460 [==============================] - 4s 10ms/step - loss: 8456039.3710\n",
      "Epoch 108/200\n",
      "460/460 [==============================] - 5s 10ms/step - loss: 12412569.2440\n",
      "Epoch 109/200\n",
      "460/460 [==============================] - 5s 10ms/step - loss: 9587859.4700\n",
      "Epoch 110/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 10005573.6346\n",
      "Epoch 111/200\n",
      "460/460 [==============================] - 4s 8ms/step - loss: 23189575.8041\n",
      "Epoch 112/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 18776690.9541\n",
      "Epoch 113/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 12859079.2108\n",
      "Epoch 114/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 20037535.1951\n",
      "Epoch 115/200\n",
      "460/460 [==============================] - 4s 8ms/step - loss: 10619191.4070\n",
      "Epoch 116/200\n",
      "460/460 [==============================] - 4s 8ms/step - loss: 20835586.3158\n",
      "Epoch 117/200\n",
      "460/460 [==============================] - 4s 8ms/step - loss: 13515051.0880\n",
      "Epoch 118/200\n",
      "460/460 [==============================] - 4s 8ms/step - loss: 11783077.2441\n",
      "Epoch 119/200\n",
      "460/460 [==============================] - 4s 8ms/step - loss: 13189198.4295\n",
      "Epoch 120/200\n",
      "460/460 [==============================] - 4s 8ms/step - loss: 10157170.9463\n",
      "Epoch 121/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 15338415.9746\n",
      "Epoch 122/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 10931306.4259\n",
      "Epoch 123/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 20159493.9811\n",
      "Epoch 124/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 11918749.7226\n",
      "Epoch 125/200\n",
      "460/460 [==============================] - 5s 10ms/step - loss: 13857528.7999\n",
      "Epoch 126/200\n",
      "460/460 [==============================] - 5s 10ms/step - loss: 16001653.2792\n",
      "Epoch 127/200\n",
      "460/460 [==============================] - 5s 10ms/step - loss: 20666204.5901\n",
      "Epoch 128/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 8764463.4340\n",
      "Epoch 129/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 11868132.8374\n",
      "Epoch 130/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 25576165.2032\n",
      "Epoch 131/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 14983531.3821\n",
      "Epoch 132/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 17453229.2177\n",
      "Epoch 133/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 11539904.9506\n",
      "Epoch 134/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 7929712.8695\n",
      "Epoch 135/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 11460555.8469\n",
      "Epoch 136/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 9838225.8056\n",
      "Epoch 137/200\n",
      "460/460 [==============================] - 4s 10ms/step - loss: 11066240.2355\n",
      "Epoch 138/200\n",
      "460/460 [==============================] - 5s 10ms/step - loss: 20541736.9188\n",
      "Epoch 139/200\n",
      "460/460 [==============================] - 5s 10ms/step - loss: 8583329.6286\n",
      "Epoch 140/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 26782239.7039\n",
      "Epoch 141/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 7763183.1153\n",
      "Epoch 142/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 14954337.3467\n",
      "Epoch 143/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 10267482.8848\n",
      "Epoch 144/200\n",
      "460/460 [==============================] - 4s 10ms/step - loss: 11600780.4660\n",
      "Epoch 145/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 26831274.0369\n",
      "Epoch 146/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 12482464.2930\n",
      "Epoch 147/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 13902145.8244\n",
      "Epoch 148/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 17995727.0444\n",
      "Epoch 149/200\n",
      "460/460 [==============================] - 4s 10ms/step - loss: 16452956.6161\n",
      "Epoch 150/200\n",
      "460/460 [==============================] - 5s 11ms/step - loss: 13806765.4473\n",
      "Epoch 151/200\n",
      "460/460 [==============================] - 5s 10ms/step - loss: 10278584.7823\n",
      "Epoch 152/200\n",
      "460/460 [==============================] - 5s 10ms/step - loss: 20292207.8369\n",
      "Epoch 153/200\n",
      "460/460 [==============================] - 4s 10ms/step - loss: 16732281.7976\n",
      "Epoch 154/200\n",
      "460/460 [==============================] - 4s 10ms/step - loss: 9117246.6602\n",
      "Epoch 155/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 11927339.3433\n",
      "Epoch 156/200\n",
      "460/460 [==============================] - 5s 10ms/step - loss: 13359838.7193\n",
      "Epoch 157/200\n",
      "460/460 [==============================] - 4s 10ms/step - loss: 7901871.9982\n",
      "Epoch 158/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 15731914.5861\n",
      "Epoch 159/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 17754350.0123\n",
      "Epoch 160/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 10227364.4790\n",
      "Epoch 161/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 7982876.7042\n",
      "Epoch 162/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 14651720.4452\n",
      "Epoch 163/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 13916253.8057\n",
      "Epoch 164/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 27742753.1260\n",
      "Epoch 165/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 9351486.5209\n",
      "Epoch 166/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 28286074.3406\n",
      "Epoch 167/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 23009556.1432\n",
      "Epoch 168/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 22945255.3942\n",
      "Epoch 169/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 19789772.6141\n",
      "Epoch 170/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 16665500.4670\n",
      "Epoch 171/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 9576630.4768\n",
      "Epoch 172/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 13812192.8243\n",
      "Epoch 173/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 19007627.7460\n",
      "Epoch 174/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 10726421.1775\n",
      "Epoch 175/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 15690091.4843\n",
      "Epoch 176/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 16722694.2598\n",
      "Epoch 177/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 8484602.8754\n",
      "Epoch 178/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 17967486.4763\n",
      "Epoch 179/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 9497323.9750\n",
      "Epoch 180/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 6227054.8877\n",
      "Epoch 181/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 13871179.2790\n",
      "Epoch 182/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460/460 [==============================] - 4s 9ms/step - loss: 18789583.8047\n",
      "Epoch 183/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 11412436.7810\n",
      "Epoch 184/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 17350890.8503\n",
      "Epoch 185/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 7542399.4926\n",
      "Epoch 186/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 9308044.0198\n",
      "Epoch 187/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 11041389.8145\n",
      "Epoch 188/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 16441167.2307\n",
      "Epoch 189/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 22431715.5177\n",
      "Epoch 190/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 14346577.3576\n",
      "Epoch 191/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 12273978.8382\n",
      "Epoch 192/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 14848405.1450\n",
      "Epoch 193/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 10724216.0499\n",
      "Epoch 194/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 16624729.6769\n",
      "Epoch 195/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 10993930.1763\n",
      "Epoch 196/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 10522495.9510\n",
      "Epoch 197/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 15129210.2990\n",
      "Epoch 198/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 15019186.8763\n",
      "Epoch 199/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 13198595.1649\n",
      "Epoch 200/200\n",
      "460/460 [==============================] - 4s 9ms/step - loss: 13956602.1321\n",
      "914.4760139957006 -0.036485359458184874 0.0\n"
     ]
    }
   ],
   "source": [
    "df_forecasting = pd.read_csv('../dataset/radar/features/weather.csv')\n",
    "coef = 0.5\n",
    "X, y = df_forecasting.iloc[:,1:], df_forecasting.iloc[:,0] \n",
    "trainX, testX = X[:int(coef * len(X))].values, X[int(coef * len(X)):].values\n",
    "trainY, testY = y[:int(coef * len(y))].values, y[int(coef * len(y)):].values\n",
    "#trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "#testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "pred = LSTM_model(trainX, trainY, testX)\n",
    "mae = metrics.mean_absolute_error(testY, pred)\n",
    "r2 = metrics.r2_score(testY, pred)\n",
    "var = metrics.explained_variance_score(testY, pred)\n",
    "print(mae, r2, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2588aecf",
   "metadata": {},
   "source": [
    "### weather(t-2), intensity(t-2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd5515d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "459/459 [==============================] - 20s 27ms/step - loss: 12811567.5783\n",
      "Epoch 2/200\n",
      "459/459 [==============================] - 13s 28ms/step - loss: 18407223.0882\n",
      "Epoch 3/200\n",
      "459/459 [==============================] - 12s 27ms/step - loss: 10909950.7609\n",
      "Epoch 4/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 12823065.7312\n",
      "Epoch 5/200\n",
      "459/459 [==============================] - 12s 27ms/step - loss: 18299147.2092\n",
      "Epoch 6/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 10608662.8364\n",
      "Epoch 7/200\n",
      "459/459 [==============================] - 12s 27ms/step - loss: 17504915.2011\n",
      "Epoch 8/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 11405412.5801\n",
      "Epoch 9/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 15007543.0009\n",
      "Epoch 10/200\n",
      "459/459 [==============================] - 11s 25ms/step - loss: 12881998.6277\n",
      "Epoch 11/200\n",
      "459/459 [==============================] - 12s 27ms/step - loss: 13435632.6920\n",
      "Epoch 12/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 32301462.2196\n",
      "Epoch 13/200\n",
      "459/459 [==============================] - 12s 27ms/step - loss: 16341450.4873\n",
      "Epoch 14/200\n",
      "459/459 [==============================] - 12s 25ms/step - loss: 18648706.4544\n",
      "Epoch 15/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 15601708.9225\n",
      "Epoch 16/200\n",
      "459/459 [==============================] - 12s 25ms/step - loss: 12703355.9240\n",
      "Epoch 17/200\n",
      "459/459 [==============================] - 11s 25ms/step - loss: 7854968.0058\n",
      "Epoch 18/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 10122600.5918\n",
      "Epoch 19/200\n",
      "459/459 [==============================] - 11s 25ms/step - loss: 19803857.3141\n",
      "Epoch 20/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 13690990.1555\n",
      "Epoch 21/200\n",
      "459/459 [==============================] - 11s 24ms/step - loss: 18992710.2692\n",
      "Epoch 22/200\n",
      "459/459 [==============================] - 12s 25ms/step - loss: 16166716.4241\n",
      "Epoch 23/200\n",
      "459/459 [==============================] - 12s 25ms/step - loss: 12296675.2677\n",
      "Epoch 24/200\n",
      "459/459 [==============================] - 11s 25ms/step - loss: 7313066.2813\n",
      "Epoch 25/200\n",
      "459/459 [==============================] - 12s 25ms/step - loss: 10004385.6884\n",
      "Epoch 26/200\n",
      "459/459 [==============================] - 11s 24ms/step - loss: 18833893.7129\n",
      "Epoch 27/200\n",
      "459/459 [==============================] - 12s 25ms/step - loss: 16607696.0292\n",
      "Epoch 28/200\n",
      "459/459 [==============================] - 11s 24ms/step - loss: 8373225.9647\n",
      "Epoch 29/200\n",
      "459/459 [==============================] - 11s 25ms/step - loss: 10492741.2708\n",
      "Epoch 30/200\n",
      "459/459 [==============================] - 11s 24ms/step - loss: 15916428.5647\n",
      "Epoch 31/200\n",
      "459/459 [==============================] - 11s 25ms/step - loss: 10853748.9303\n",
      "Epoch 32/200\n",
      "459/459 [==============================] - 11s 24ms/step - loss: 11375569.0530\n",
      "Epoch 33/200\n",
      "459/459 [==============================] - 11s 25ms/step - loss: 26149856.6674\n",
      "Epoch 34/200\n",
      "459/459 [==============================] - 11s 25ms/step - loss: 8764251.2341\n",
      "Epoch 35/200\n",
      "459/459 [==============================] - 12s 25ms/step - loss: 16321166.8849\n",
      "Epoch 36/200\n",
      "459/459 [==============================] - 11s 25ms/step - loss: 21031032.1845\n",
      "Epoch 37/200\n",
      "459/459 [==============================] - 13s 27ms/step - loss: 15262712.3086\n",
      "Epoch 38/200\n",
      "459/459 [==============================] - 13s 27ms/step - loss: 11671889.0149\n",
      "Epoch 39/200\n",
      "459/459 [==============================] - 13s 28ms/step - loss: 24385721.1204\n",
      "Epoch 40/200\n",
      "459/459 [==============================] - 12s 27ms/step - loss: 7546592.5384\n",
      "Epoch 41/200\n",
      "459/459 [==============================] - 13s 27ms/step - loss: 13165349.5748\n",
      "Epoch 42/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 12488962.9436\n",
      "Epoch 43/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 10282386.5292\n",
      "Epoch 44/200\n",
      "459/459 [==============================] - 12s 25ms/step - loss: 16592330.8821\n",
      "Epoch 45/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 15060640.0464\n",
      "Epoch 46/200\n",
      "459/459 [==============================] - 12s 27ms/step - loss: 9011476.3831\n",
      "Epoch 47/200\n",
      "459/459 [==============================] - 13s 28ms/step - loss: 9274677.1316\n",
      "Epoch 48/200\n",
      "459/459 [==============================] - 13s 27ms/step - loss: 10164906.0874\n",
      "Epoch 49/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 8911029.2260\n",
      "Epoch 50/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 12329447.4393\n",
      "Epoch 51/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 21611656.3946\n",
      "Epoch 52/200\n",
      "459/459 [==============================] - 13s 27ms/step - loss: 19002265.1252\n",
      "Epoch 53/200\n",
      "459/459 [==============================] - 13s 27ms/step - loss: 11706554.3764\n",
      "Epoch 54/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 16019319.1929\n",
      "Epoch 55/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 10928695.2312\n",
      "Epoch 56/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 15891296.5893\n",
      "Epoch 57/200\n",
      "459/459 [==============================] - 13s 28ms/step - loss: 8730012.5428\n",
      "Epoch 58/200\n",
      "459/459 [==============================] - 12s 27ms/step - loss: 13071276.8044\n",
      "Epoch 59/200\n",
      "459/459 [==============================] - 13s 29ms/step - loss: 13860908.7102\n",
      "Epoch 60/200\n",
      "459/459 [==============================] - 13s 28ms/step - loss: 18818265.6104\n",
      "Epoch 61/200\n",
      "459/459 [==============================] - 13s 27ms/step - loss: 8502349.6861\n",
      "Epoch 62/200\n",
      "459/459 [==============================] - 14s 30ms/step - loss: 12066900.8680\n",
      "Epoch 63/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 14307239.4951\n",
      "Epoch 64/200\n",
      "459/459 [==============================] - 13s 29ms/step - loss: 25561580.4181\n",
      "Epoch 65/200\n",
      "459/459 [==============================] - 13s 27ms/step - loss: 14675524.5886\n",
      "Epoch 66/200\n",
      "459/459 [==============================] - 13s 28ms/step - loss: 11617257.5129\n",
      "Epoch 67/200\n",
      "459/459 [==============================] - 12s 27ms/step - loss: 23733696.5854\n",
      "Epoch 68/200\n",
      "459/459 [==============================] - 13s 28ms/step - loss: 22004988.8043\n",
      "Epoch 69/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 18310992.7776\n",
      "Epoch 70/200\n",
      "459/459 [==============================] - 12s 27ms/step - loss: 11392149.3683\n",
      "Epoch 71/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 22162643.1644\n",
      "Epoch 72/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 10129223.8257\n",
      "Epoch 73/200\n",
      "459/459 [==============================] - 12s 27ms/step - loss: 15745989.3772\n",
      "Epoch 74/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 18922145.4365\n",
      "Epoch 75/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 12810873.8358\n",
      "Epoch 76/200\n",
      "459/459 [==============================] - 13s 29ms/step - loss: 7858021.4500\n",
      "Epoch 77/200\n",
      "459/459 [==============================] - 13s 28ms/step - loss: 18921110.2473\n",
      "Epoch 78/200\n",
      "459/459 [==============================] - 13s 27ms/step - loss: 14544177.0700\n",
      "Epoch 79/200\n",
      "459/459 [==============================] - 11s 24ms/step - loss: 6284850.8191\n",
      "Epoch 80/200\n",
      "459/459 [==============================] - 11s 25ms/step - loss: 13939488.0720\n",
      "Epoch 81/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 9187326.2887\n",
      "Epoch 82/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 14904581.5068\n",
      "Epoch 83/200\n",
      "459/459 [==============================] - 12s 27ms/step - loss: 13273669.9781\n",
      "Epoch 84/200\n",
      "459/459 [==============================] - 12s 27ms/step - loss: 5316924.5057\n",
      "Epoch 85/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 11408895.5148\n",
      "Epoch 86/200\n",
      "459/459 [==============================] - 12s 27ms/step - loss: 33673675.2087\n",
      "Epoch 87/200\n",
      "459/459 [==============================] - 12s 25ms/step - loss: 14369623.0453\n",
      "Epoch 88/200\n",
      "459/459 [==============================] - 12s 27ms/step - loss: 8260412.8196\n",
      "Epoch 89/200\n",
      "459/459 [==============================] - 12s 27ms/step - loss: 15879759.2906\n",
      "Epoch 90/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "459/459 [==============================] - 12s 27ms/step - loss: 10278149.7957\n",
      "Epoch 91/200\n",
      "459/459 [==============================] - 12s 27ms/step - loss: 13014572.2501\n",
      "Epoch 92/200\n",
      "459/459 [==============================] - 12s 25ms/step - loss: 11692005.2201\n",
      "Epoch 93/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 20088407.7667\n",
      "Epoch 94/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 21949771.9546\n",
      "Epoch 95/200\n",
      "459/459 [==============================] - 12s 27ms/step - loss: 11255209.0453\n",
      "Epoch 96/200\n",
      "459/459 [==============================] - 13s 28ms/step - loss: 17205839.3668\n",
      "Epoch 97/200\n",
      "459/459 [==============================] - 12s 27ms/step - loss: 14748112.4348\n",
      "Epoch 98/200\n",
      "459/459 [==============================] - 13s 28ms/step - loss: 11844525.1708\n",
      "Epoch 99/200\n",
      "459/459 [==============================] - 13s 28ms/step - loss: 11267444.7081\n",
      "Epoch 100/200\n",
      "459/459 [==============================] - 11s 25ms/step - loss: 9518623.8840\n",
      "Epoch 101/200\n",
      "459/459 [==============================] - 13s 28ms/step - loss: 9201630.0633\n",
      "Epoch 102/200\n",
      "459/459 [==============================] - 11s 25ms/step - loss: 10770935.2523\n",
      "Epoch 103/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 13715898.2929\n",
      "Epoch 104/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 12860524.7178\n",
      "Epoch 105/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 9699699.4297\n",
      "Epoch 106/200\n",
      "459/459 [==============================] - 11s 25ms/step - loss: 10359396.6455\n",
      "Epoch 107/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 10647129.5655\n",
      "Epoch 108/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 12020331.6661\n",
      "Epoch 109/200\n",
      "459/459 [==============================] - 12s 25ms/step - loss: 28771778.2457\n",
      "Epoch 110/200\n",
      "459/459 [==============================] - 11s 25ms/step - loss: 6215647.0441\n",
      "Epoch 111/200\n",
      "459/459 [==============================] - 12s 25ms/step - loss: 26013085.2048\n",
      "Epoch 112/200\n",
      "459/459 [==============================] - 11s 25ms/step - loss: 14672915.3508\n",
      "Epoch 113/200\n",
      "459/459 [==============================] - 11s 25ms/step - loss: 12236035.1402\n",
      "Epoch 114/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 18313996.2322\n",
      "Epoch 115/200\n",
      "459/459 [==============================] - 11s 24ms/step - loss: 8565128.7749\n",
      "Epoch 116/200\n",
      "459/459 [==============================] - 11s 23ms/step - loss: 12158655.8924\n",
      "Epoch 117/200\n",
      "459/459 [==============================] - 11s 24ms/step - loss: 12227796.9778\n",
      "Epoch 118/200\n",
      "459/459 [==============================] - 11s 23ms/step - loss: 13707116.0671\n",
      "Epoch 119/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 8035903.7604\n",
      "Epoch 120/200\n",
      "459/459 [==============================] - 12s 27ms/step - loss: 12387425.6095\n",
      "Epoch 121/200\n",
      "459/459 [==============================] - 12s 27ms/step - loss: 16762865.4261\n",
      "Epoch 122/200\n",
      "459/459 [==============================] - 13s 28ms/step - loss: 9093519.2251\n",
      "Epoch 123/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 13651885.2821\n",
      "Epoch 124/200\n",
      "459/459 [==============================] - 12s 27ms/step - loss: 8388224.9531\n",
      "Epoch 125/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 14120031.7045\n",
      "Epoch 126/200\n",
      "459/459 [==============================] - 12s 27ms/step - loss: 9080945.3598\n",
      "Epoch 127/200\n",
      "459/459 [==============================] - 12s 25ms/step - loss: 10884073.3374\n",
      "Epoch 128/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 18553663.9098\n",
      "Epoch 129/200\n",
      "459/459 [==============================] - 11s 25ms/step - loss: 9992651.1721\n",
      "Epoch 130/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 12410103.1451\n",
      "Epoch 131/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 8371239.6346\n",
      "Epoch 132/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 27521789.4913\n",
      "Epoch 133/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 12918323.1832\n",
      "Epoch 134/200\n",
      "459/459 [==============================] - 12s 25ms/step - loss: 17439108.8505\n",
      "Epoch 135/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 13921025.1048\n",
      "Epoch 136/200\n",
      "459/459 [==============================] - 12s 25ms/step - loss: 6050667.9526\n",
      "Epoch 137/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 10658582.0454\n",
      "Epoch 138/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 10219889.4002\n",
      "Epoch 139/200\n",
      "459/459 [==============================] - 12s 25ms/step - loss: 8986209.6212\n",
      "Epoch 140/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 8331664.8886\n",
      "Epoch 141/200\n",
      "459/459 [==============================] - 12s 25ms/step - loss: 25879997.8500\n",
      "Epoch 142/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 14988079.3254\n",
      "Epoch 143/200\n",
      "459/459 [==============================] - 12s 25ms/step - loss: 13301035.7749\n",
      "Epoch 144/200\n",
      "459/459 [==============================] - 12s 25ms/step - loss: 7572585.7889\n",
      "Epoch 145/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 10221661.3376\n",
      "Epoch 146/200\n",
      "459/459 [==============================] - 12s 25ms/step - loss: 16300893.4362\n",
      "Epoch 147/200\n",
      "459/459 [==============================] - 12s 25ms/step - loss: 19935964.6058\n",
      "Epoch 148/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 15583952.4949\n",
      "Epoch 149/200\n",
      "459/459 [==============================] - 11s 25ms/step - loss: 14589162.4549\n",
      "Epoch 150/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 23128767.0294\n",
      "Epoch 151/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 9235283.4930\n",
      "Epoch 152/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 8818651.8537\n",
      "Epoch 153/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 13990289.0457\n",
      "Epoch 154/200\n",
      "459/459 [==============================] - 12s 25ms/step - loss: 17380564.9287\n",
      "Epoch 155/200\n",
      "459/459 [==============================] - 12s 27ms/step - loss: 22575647.3257\n",
      "Epoch 156/200\n",
      "459/459 [==============================] - 12s 27ms/step - loss: 14542866.3336\n",
      "Epoch 157/200\n",
      "459/459 [==============================] - 12s 25ms/step - loss: 8020569.7725\n",
      "Epoch 158/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 13636935.3347\n",
      "Epoch 159/200\n",
      "459/459 [==============================] - 12s 25ms/step - loss: 15978134.5014\n",
      "Epoch 160/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 14425771.9978\n",
      "Epoch 161/200\n",
      "459/459 [==============================] - 12s 25ms/step - loss: 29552003.4055\n",
      "Epoch 162/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 6901846.8440\n",
      "Epoch 163/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 7135836.8752\n",
      "Epoch 164/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 17290231.8403\n",
      "Epoch 165/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 8980596.6621\n",
      "Epoch 166/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 8377524.9801\n",
      "Epoch 167/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 5774959.4369\n",
      "Epoch 168/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 18942410.8402\n",
      "Epoch 169/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 20351381.5788\n",
      "Epoch 170/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 9738116.2585\n",
      "Epoch 171/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 15586076.4075\n",
      "Epoch 172/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 15330706.3382\n",
      "Epoch 173/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 12597119.0318\n",
      "Epoch 174/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 14410181.3727\n",
      "Epoch 175/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 8044446.0170\n",
      "Epoch 176/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 7685142.2310\n",
      "Epoch 177/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 19161777.4043\n",
      "Epoch 178/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "459/459 [==============================] - 12s 26ms/step - loss: 12215723.3404\n",
      "Epoch 179/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 24358268.1113\n",
      "Epoch 180/200\n",
      "459/459 [==============================] - 12s 25ms/step - loss: 12361860.9367\n",
      "Epoch 181/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 17880634.6660\n",
      "Epoch 182/200\n",
      "459/459 [==============================] - 12s 25ms/step - loss: 7220862.2038\n",
      "Epoch 183/200\n",
      "459/459 [==============================] - 12s 25ms/step - loss: 19559979.7463\n",
      "Epoch 184/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 20618832.5241\n",
      "Epoch 185/200\n",
      "459/459 [==============================] - 12s 25ms/step - loss: 13608761.5802\n",
      "Epoch 186/200\n",
      "459/459 [==============================] - 12s 27ms/step - loss: 12223165.5192\n",
      "Epoch 187/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 20506120.4261\n",
      "Epoch 188/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 16145434.0050\n",
      "Epoch 189/200\n",
      "459/459 [==============================] - 11s 25ms/step - loss: 16389960.1666\n",
      "Epoch 190/200\n",
      "459/459 [==============================] - 12s 27ms/step - loss: 14109550.9043\n",
      "Epoch 191/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 9645270.7906\n",
      "Epoch 192/200\n",
      "459/459 [==============================] - 12s 27ms/step - loss: 13975573.2449\n",
      "Epoch 193/200\n",
      "459/459 [==============================] - 12s 27ms/step - loss: 11196017.6251\n",
      "Epoch 194/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 13931061.4713\n",
      "Epoch 195/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 18519143.2226\n",
      "Epoch 196/200\n",
      "459/459 [==============================] - 12s 27ms/step - loss: 13447798.0427\n",
      "Epoch 197/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 13588460.1763\n",
      "Epoch 198/200\n",
      "459/459 [==============================] - 12s 27ms/step - loss: 11980613.4677\n",
      "Epoch 199/200\n",
      "459/459 [==============================] - 12s 26ms/step - loss: 12940645.7625\n",
      "Epoch 200/200\n",
      "459/459 [==============================] - 12s 27ms/step - loss: 8085384.0940\n",
      "914.7146952057142 -0.03704927111363476 -1.520228387619227e-11\n"
     ]
    }
   ],
   "source": [
    "df_forecasting = pd.read_csv('../dataset/radar/features/weather_lags2.csv')\n",
    "coef = 0.5\n",
    "X, y = df_forecasting.iloc[:,1:], df_forecasting.iloc[:,0] \n",
    "trainX, testX = X[:int(coef * len(X))].values, X[int(coef * len(X)):].values\n",
    "trainY, testY = y[:int(coef * len(y))].values, y[int(coef * len(y)):].values\n",
    "#trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "#testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "pred = LSTM_model(trainX, trainY, testX)\n",
    "mae = metrics.mean_absolute_error(testY, pred)\n",
    "r2 = metrics.r2_score(testY, pred)\n",
    "var = metrics.explained_variance_score(testY, pred)\n",
    "print(mae, r2, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeab2b0",
   "metadata": {},
   "source": [
    "### weather(t-2), intensity(t-2) selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91bb35e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "459/459 [==============================] - 12s 8ms/step - loss: 14112079.1924\n",
      "Epoch 2/200\n",
      "459/459 [==============================] - 4s 9ms/step - loss: 9533841.0970\n",
      "Epoch 3/200\n",
      "459/459 [==============================] - 5s 10ms/step - loss: 8005889.0091\n",
      "Epoch 4/200\n",
      "459/459 [==============================] - 4s 9ms/step - loss: 21693770.1027\n",
      "Epoch 5/200\n",
      "459/459 [==============================] - 4s 9ms/step - loss: 18403911.2735\n",
      "Epoch 6/200\n",
      "459/459 [==============================] - 4s 9ms/step - loss: 9960423.8651\n",
      "Epoch 7/200\n",
      "459/459 [==============================] - 4s 9ms/step - loss: 12720139.0076\n",
      "Epoch 8/200\n",
      "459/459 [==============================] - 4s 9ms/step - loss: 10269740.4792\n",
      "Epoch 9/200\n",
      "459/459 [==============================] - 4s 9ms/step - loss: 14347078.9052\n",
      "Epoch 10/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 7429930.3963\n",
      "Epoch 11/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 17374200.8590\n",
      "Epoch 12/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 15347385.9655\n",
      "Epoch 13/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 15352072.0403\n",
      "Epoch 14/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 25439747.0565\n",
      "Epoch 15/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 10904660.9314\n",
      "Epoch 16/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 7608950.5571\n",
      "Epoch 17/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 9496623.3293\n",
      "Epoch 18/200\n",
      "459/459 [==============================] - 4s 9ms/step - loss: 20987258.9011\n",
      "Epoch 19/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 18055545.5059\n",
      "Epoch 20/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 11454811.0327\n",
      "Epoch 21/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 10265567.5834\n",
      "Epoch 22/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 8245795.1728\n",
      "Epoch 23/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 16275981.1493\n",
      "Epoch 24/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 13145753.9000\n",
      "Epoch 25/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 10682040.9129\n",
      "Epoch 26/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 18168473.7280\n",
      "Epoch 27/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 7842752.4749\n",
      "Epoch 28/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 10920674.6721\n",
      "Epoch 29/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 12626487.6487\n",
      "Epoch 30/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 28647405.1255\n",
      "Epoch 31/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 14280727.9122\n",
      "Epoch 32/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 16354014.2952\n",
      "Epoch 33/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 13686458.1079\n",
      "Epoch 34/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 7153327.0341\n",
      "Epoch 35/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 15140275.1795\n",
      "Epoch 36/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 11012600.7119\n",
      "Epoch 37/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 16162716.1096\n",
      "Epoch 38/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 11664204.5596\n",
      "Epoch 39/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 24708979.0159\n",
      "Epoch 40/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 12402095.5807\n",
      "Epoch 41/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 8304603.3390\n",
      "Epoch 42/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 18818033.0581\n",
      "Epoch 43/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 10046137.2092\n",
      "Epoch 44/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 20308312.5948\n",
      "Epoch 45/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 10421383.4199\n",
      "Epoch 46/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 14794004.1020\n",
      "Epoch 47/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 20202080.2959\n",
      "Epoch 48/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 7110105.5452\n",
      "Epoch 49/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 15776237.2330\n",
      "Epoch 50/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 9882577.7748\n",
      "Epoch 51/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 18263488.6348\n",
      "Epoch 52/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 17582224.2399\n",
      "Epoch 53/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 13501450.3062\n",
      "Epoch 54/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 15557215.1625\n",
      "Epoch 55/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 14672543.3558\n",
      "Epoch 56/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 10844633.4888\n",
      "Epoch 57/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 7273248.3082\n",
      "Epoch 58/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 11839664.7448\n",
      "Epoch 59/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 16705393.5796\n",
      "Epoch 60/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 12881650.2237\n",
      "Epoch 61/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 13592512.2797\n",
      "Epoch 62/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 12183758.9130\n",
      "Epoch 63/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 12041487.7260\n",
      "Epoch 64/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 9623169.9769\n",
      "Epoch 65/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 25911817.6193\n",
      "Epoch 66/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 10486288.9559\n",
      "Epoch 67/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 17963906.0612\n",
      "Epoch 68/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 14309575.9749\n",
      "Epoch 69/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 12634009.0869\n",
      "Epoch 70/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 17819459.6860\n",
      "Epoch 71/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 8705366.1869\n",
      "Epoch 72/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 14588865.4721\n",
      "Epoch 73/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 13424602.0298\n",
      "Epoch 74/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 16767620.5868\n",
      "Epoch 75/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 13388704.4339\n",
      "Epoch 76/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 8481714.0167\n",
      "Epoch 77/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 13614304.8806\n",
      "Epoch 78/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 24369783.5578\n",
      "Epoch 79/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 16081287.3125\n",
      "Epoch 80/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 22717112.7660\n",
      "Epoch 81/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 23727204.1282\n",
      "Epoch 82/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 12425456.8876\n",
      "Epoch 83/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 9150906.1330\n",
      "Epoch 84/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 14184196.9299\n",
      "Epoch 85/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 20223605.4855\n",
      "Epoch 86/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 12552289.7194\n",
      "Epoch 87/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 10703074.8019\n",
      "Epoch 88/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 8714240.3507\n",
      "Epoch 89/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 12919713.9017\n",
      "Epoch 90/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 19407742.9815\n",
      "Epoch 91/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 14756169.2402\n",
      "Epoch 92/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "459/459 [==============================] - 4s 8ms/step - loss: 9447383.0085\n",
      "Epoch 93/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 15156952.2853\n",
      "Epoch 94/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 14771972.2757\n",
      "Epoch 95/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 10349426.7654\n",
      "Epoch 96/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 6699462.9537\n",
      "Epoch 97/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 11233324.1720\n",
      "Epoch 98/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 13672855.0434\n",
      "Epoch 99/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 12973216.0704\n",
      "Epoch 100/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 10729284.8616\n",
      "Epoch 101/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 12014537.1206\n",
      "Epoch 102/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 8979959.4210\n",
      "Epoch 103/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 22916321.6072\n",
      "Epoch 104/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 22703374.8960\n",
      "Epoch 105/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 7714348.4862\n",
      "Epoch 106/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 15702413.6763\n",
      "Epoch 107/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 16940845.3016\n",
      "Epoch 108/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 16054271.8652\n",
      "Epoch 109/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 8263816.1161\n",
      "Epoch 110/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 10718100.0462\n",
      "Epoch 111/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 12240957.4532\n",
      "Epoch 112/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 10491331.0827\n",
      "Epoch 113/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 12005144.1774\n",
      "Epoch 114/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 14489727.3681\n",
      "Epoch 115/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 16467277.9789\n",
      "Epoch 116/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 10498648.8790\n",
      "Epoch 117/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 20508536.2515\n",
      "Epoch 118/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 11807609.5425\n",
      "Epoch 119/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 9653063.8135\n",
      "Epoch 120/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 8331838.0216\n",
      "Epoch 121/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 10148892.1584\n",
      "Epoch 122/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 10653397.5210\n",
      "Epoch 123/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 12832344.8674\n",
      "Epoch 124/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 11089295.1074\n",
      "Epoch 125/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 11022102.5433\n",
      "Epoch 126/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 15484890.6736\n",
      "Epoch 127/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 19984887.8649\n",
      "Epoch 128/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 11915806.1069\n",
      "Epoch 129/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 7688208.2381\n",
      "Epoch 130/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 9561167.6728\n",
      "Epoch 131/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 9284986.2565\n",
      "Epoch 132/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 13326436.5582\n",
      "Epoch 133/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 16603346.4738\n",
      "Epoch 134/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 15628194.3997\n",
      "Epoch 135/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 11257667.4652\n",
      "Epoch 136/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 12644097.2454\n",
      "Epoch 137/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 13756247.9009\n",
      "Epoch 138/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 11649319.5471\n",
      "Epoch 139/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 17191014.9969\n",
      "Epoch 140/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 11538360.4501\n",
      "Epoch 141/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 8173354.7518\n",
      "Epoch 142/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 5525145.7571\n",
      "Epoch 143/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 12232178.1118\n",
      "Epoch 144/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 9169031.5620\n",
      "Epoch 145/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 8178709.7799\n",
      "Epoch 146/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 11576244.4981\n",
      "Epoch 147/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 10442817.8060\n",
      "Epoch 148/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 10093115.9641\n",
      "Epoch 149/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 25158505.1739\n",
      "Epoch 150/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 27277323.0717\n",
      "Epoch 151/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 9682672.6887\n",
      "Epoch 152/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 7690327.7856\n",
      "Epoch 153/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 9917279.8879\n",
      "Epoch 154/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 12545480.1401\n",
      "Epoch 155/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 10725787.8832\n",
      "Epoch 156/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 8454317.7366\n",
      "Epoch 157/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 8133348.7519\n",
      "Epoch 158/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 7796833.3014\n",
      "Epoch 159/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 16389038.1318\n",
      "Epoch 160/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 16154466.5772\n",
      "Epoch 161/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 12039756.4288\n",
      "Epoch 162/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 13627834.5094\n",
      "Epoch 163/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 10485807.6047\n",
      "Epoch 164/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 17186755.8521\n",
      "Epoch 165/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 11244142.7764\n",
      "Epoch 166/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 11747447.3790\n",
      "Epoch 167/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 16546977.9853\n",
      "Epoch 168/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 23546394.5710\n",
      "Epoch 169/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 21745088.0571\n",
      "Epoch 170/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 10643070.9955\n",
      "Epoch 171/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 10656489.3561\n",
      "Epoch 172/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 10969738.6017\n",
      "Epoch 173/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 23697882.0627\n",
      "Epoch 174/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 10382591.6427\n",
      "Epoch 175/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 13319178.9885\n",
      "Epoch 176/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 10744858.3005\n",
      "Epoch 177/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 11274613.5844\n",
      "Epoch 178/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 12598598.9416\n",
      "Epoch 179/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 11570035.1510\n",
      "Epoch 180/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 17236378.2015\n",
      "Epoch 181/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 11210594.1962\n",
      "Epoch 182/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "459/459 [==============================] - 4s 8ms/step - loss: 10520873.5432\n",
      "Epoch 183/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 8759390.5778\n",
      "Epoch 184/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 13571623.7528\n",
      "Epoch 185/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 10353476.4512\n",
      "Epoch 186/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 9410542.5503\n",
      "Epoch 187/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 16716808.1138\n",
      "Epoch 188/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 13654110.0747\n",
      "Epoch 189/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 9424984.7577\n",
      "Epoch 190/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 12955603.1869\n",
      "Epoch 191/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 8002258.1101\n",
      "Epoch 192/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 18628691.9625\n",
      "Epoch 193/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 13880460.7919\n",
      "Epoch 194/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 10607876.6423\n",
      "Epoch 195/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 10980168.1128\n",
      "Epoch 196/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 10919075.7552\n",
      "Epoch 197/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 12875649.2941\n",
      "Epoch 198/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 15282412.0353\n",
      "Epoch 199/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 15519177.7632\n",
      "Epoch 200/200\n",
      "459/459 [==============================] - 4s 8ms/step - loss: 27513545.2424\n",
      "817.4269664906949 -0.032364265431222616 0.021524137362203155\n"
     ]
    }
   ],
   "source": [
    "df_forecasting = pd.read_csv('../dataset/radar/features/weather_lags2_selected.csv')\n",
    "coef = 0.5\n",
    "X, y = df_forecasting.iloc[:,1:], df_forecasting.iloc[:,0] \n",
    "trainX, testX = X[:int(coef * len(X))].values, X[int(coef * len(X)):].values\n",
    "trainY, testY = y[:int(coef * len(y))].values, y[int(coef * len(y)):].values\n",
    "#trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "#testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "pred = LSTM_model(trainX, trainY, testX)\n",
    "mae = metrics.mean_absolute_error(testY, pred)\n",
    "r2 = metrics.r2_score(testY, pred)\n",
    "var = metrics.explained_variance_score(testY, pred)\n",
    "print(mae, r2, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49770a4",
   "metadata": {},
   "source": [
    "### weather(t-5), intensity(t-5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "320ca890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "457/457 [==============================] - 12s 8ms/step - loss: 16112452.4188\n",
      "Epoch 2/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 21207229.5183\n",
      "Epoch 3/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 12381748.2092\n",
      "Epoch 4/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 13792822.1622\n",
      "Epoch 5/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 16220510.3584\n",
      "Epoch 6/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 17230562.5259\n",
      "Epoch 7/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 18413316.4351\n",
      "Epoch 8/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 11628741.3204\n",
      "Epoch 9/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 11418283.2511\n",
      "Epoch 10/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 15826575.3394\n",
      "Epoch 11/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 17835903.4748\n",
      "Epoch 12/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 13560500.3281\n",
      "Epoch 13/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 10671844.8713\n",
      "Epoch 14/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 9444310.9835\n",
      "Epoch 15/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 12143652.0775\n",
      "Epoch 16/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 15265061.2738\n",
      "Epoch 17/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 17153660.8115\n",
      "Epoch 18/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 10566273.4809\n",
      "Epoch 19/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 6555208.9839\n",
      "Epoch 20/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 17662920.1239\n",
      "Epoch 21/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 12571764.8605\n",
      "Epoch 22/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 21937688.8999\n",
      "Epoch 23/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 12922440.7765\n",
      "Epoch 24/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 6423322.4771\n",
      "Epoch 25/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 15905713.7100\n",
      "Epoch 26/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 11040206.5379\n",
      "Epoch 27/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 9251182.3152\n",
      "Epoch 28/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 5993986.1282\n",
      "Epoch 29/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 12611029.0081\n",
      "Epoch 30/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 12296240.1412\n",
      "Epoch 31/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 8404954.3392\n",
      "Epoch 32/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 7662155.2293\n",
      "Epoch 33/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 11808468.7826\n",
      "Epoch 34/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 15532404.3710\n",
      "Epoch 35/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 14185305.2700\n",
      "Epoch 36/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 25571028.8477\n",
      "Epoch 37/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 18591956.5542\n",
      "Epoch 38/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 12278999.8417\n",
      "Epoch 39/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 11727405.2781\n",
      "Epoch 40/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 16584490.8485\n",
      "Epoch 41/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 21370110.8330\n",
      "Epoch 42/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 18836659.8863\n",
      "Epoch 43/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 11519495.2829\n",
      "Epoch 44/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 11044947.4751\n",
      "Epoch 45/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 13943676.1966\n",
      "Epoch 46/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 14644659.7497\n",
      "Epoch 47/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 14884920.4173\n",
      "Epoch 48/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 13321644.4707\n",
      "Epoch 49/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 17285091.1706\n",
      "Epoch 50/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 11080238.0730\n",
      "Epoch 51/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 9739298.6559\n",
      "Epoch 52/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 7426950.8805\n",
      "Epoch 53/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 16145831.5361\n",
      "Epoch 54/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 12417791.4412\n",
      "Epoch 55/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 17817809.9707\n",
      "Epoch 56/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 11350833.3148\n",
      "Epoch 57/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 9131521.5227\n",
      "Epoch 58/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 8608862.4568\n",
      "Epoch 59/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 14016640.9476\n",
      "Epoch 60/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 14297514.7612\n",
      "Epoch 61/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 16754694.3229\n",
      "Epoch 62/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 19312960.5662\n",
      "Epoch 63/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 13725146.4066\n",
      "Epoch 64/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 11564364.6694\n",
      "Epoch 65/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 12007110.5735\n",
      "Epoch 66/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 10389673.9865\n",
      "Epoch 67/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 14877688.1890\n",
      "Epoch 68/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 12772169.9039\n",
      "Epoch 69/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 19977362.4773\n",
      "Epoch 70/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 10060754.5787\n",
      "Epoch 71/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 24078806.9058\n",
      "Epoch 72/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 17080855.9141\n",
      "Epoch 73/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 12313094.1835\n",
      "Epoch 74/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 11803359.0768\n",
      "Epoch 75/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 16104372.9058\n",
      "Epoch 76/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 17989197.3684\n",
      "Epoch 77/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 14697872.3621\n",
      "Epoch 78/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 15449523.8177\n",
      "Epoch 79/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 13307300.8383\n",
      "Epoch 80/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 14501532.9745\n",
      "Epoch 81/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 20485751.6787\n",
      "Epoch 82/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 15290270.4606\n",
      "Epoch 83/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 12593560.7432\n",
      "Epoch 84/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 19212279.0936\n",
      "Epoch 85/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 8788364.3893\n",
      "Epoch 86/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 11105609.6618\n",
      "Epoch 87/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 11718639.1616\n",
      "Epoch 88/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 15191757.3881\n",
      "Epoch 89/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 16906280.8267\n",
      "Epoch 90/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 15225763.2220\n",
      "Epoch 91/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 15205343.1478\n",
      "Epoch 92/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "457/457 [==============================] - 4s 8ms/step - loss: 11248156.3680\n",
      "Epoch 93/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 11641052.0485\n",
      "Epoch 94/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 14698844.1093\n",
      "Epoch 95/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 11235291.6479\n",
      "Epoch 96/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 15886245.6365\n",
      "Epoch 97/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 11743623.8432\n",
      "Epoch 98/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 10087692.4944\n",
      "Epoch 99/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 9851204.8813\n",
      "Epoch 100/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 17551933.9253\n",
      "Epoch 101/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 18664562.4246\n",
      "Epoch 102/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 7034868.4000\n",
      "Epoch 103/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 10301434.7417\n",
      "Epoch 104/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 13216502.9331\n",
      "Epoch 105/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 11159280.2131\n",
      "Epoch 106/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 14277232.4374\n",
      "Epoch 107/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 13343760.5928\n",
      "Epoch 108/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 17070494.6813\n",
      "Epoch 109/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 10237309.8691\n",
      "Epoch 110/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 10423759.5041\n",
      "Epoch 111/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 13660194.5408\n",
      "Epoch 112/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 11564910.3101\n",
      "Epoch 113/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 14062119.1534\n",
      "Epoch 114/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 15740209.0016\n",
      "Epoch 115/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 13778131.2403\n",
      "Epoch 116/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 19131836.1621\n",
      "Epoch 117/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 13757171.9083\n",
      "Epoch 118/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 10342281.4106\n",
      "Epoch 119/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 12557455.8409\n",
      "Epoch 120/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 10710662.5513\n",
      "Epoch 121/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 7912775.4661\n",
      "Epoch 122/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 17041070.8365\n",
      "Epoch 123/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 8288020.7693\n",
      "Epoch 124/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 10800909.0675\n",
      "Epoch 125/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 15928898.0065\n",
      "Epoch 126/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 11291854.8302\n",
      "Epoch 127/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 19068127.9459\n",
      "Epoch 128/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 14039552.5712\n",
      "Epoch 129/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 12584775.6189\n",
      "Epoch 130/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 17409477.7943\n",
      "Epoch 131/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 20196950.0175\n",
      "Epoch 132/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 11829097.7851\n",
      "Epoch 133/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 22850694.4628\n",
      "Epoch 134/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 11380428.0310\n",
      "Epoch 135/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 13072628.2709\n",
      "Epoch 136/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 10894893.5867\n",
      "Epoch 137/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 3945828.9766\n",
      "Epoch 138/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 11994197.9834\n",
      "Epoch 139/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 15534114.9520\n",
      "Epoch 140/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 20049943.4454\n",
      "Epoch 141/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 12876412.0524\n",
      "Epoch 142/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 8670875.5395\n",
      "Epoch 143/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 18272891.1670\n",
      "Epoch 144/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 14907391.0632\n",
      "Epoch 145/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 10068757.4307\n",
      "Epoch 146/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 16473319.2903\n",
      "Epoch 147/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 13999201.4849\n",
      "Epoch 148/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 10470063.8629\n",
      "Epoch 149/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 14509706.1488\n",
      "Epoch 150/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 8573689.4172\n",
      "Epoch 151/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 12033171.4743\n",
      "Epoch 152/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 7159039.7165\n",
      "Epoch 153/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 14404280.6817\n",
      "Epoch 154/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 17355922.2183\n",
      "Epoch 155/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 9021748.7181\n",
      "Epoch 156/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 14422643.6664\n",
      "Epoch 157/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 14598365.9941\n",
      "Epoch 158/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 17206767.2517\n",
      "Epoch 159/200\n",
      "457/457 [==============================] - 4s 10ms/step - loss: 12572624.0799\n",
      "Epoch 160/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 11522432.4584\n",
      "Epoch 161/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 15373210.2999\n",
      "Epoch 162/200\n",
      "457/457 [==============================] - 4s 10ms/step - loss: 15103514.9336\n",
      "Epoch 163/200\n",
      "457/457 [==============================] - 4s 10ms/step - loss: 8614691.0496\n",
      "Epoch 164/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 15517078.1566\n",
      "Epoch 165/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 17620348.6479\n",
      "Epoch 166/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 15925338.9078\n",
      "Epoch 167/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 10389810.7994\n",
      "Epoch 168/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 12490521.5732\n",
      "Epoch 169/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 17883502.1699\n",
      "Epoch 170/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 18299347.2345\n",
      "Epoch 171/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 14117475.7609\n",
      "Epoch 172/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 12944973.4180\n",
      "Epoch 173/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 14892621.4831\n",
      "Epoch 174/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 10165066.7841\n",
      "Epoch 175/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 11778287.0507\n",
      "Epoch 176/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 17061345.4810\n",
      "Epoch 177/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 7388443.3676\n",
      "Epoch 178/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 6919571.9000\n",
      "Epoch 179/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 13396472.7915\n",
      "Epoch 180/200\n",
      "457/457 [==============================] - 4s 10ms/step - loss: 9059398.8583\n",
      "Epoch 181/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 11567466.3706\n",
      "Epoch 182/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "457/457 [==============================] - 4s 8ms/step - loss: 13493791.5445\n",
      "Epoch 183/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 11035732.2250\n",
      "Epoch 184/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 18561065.5535\n",
      "Epoch 185/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 13549586.0560\n",
      "Epoch 186/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 14851833.8826\n",
      "Epoch 187/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 14770128.5096\n",
      "Epoch 188/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 13779818.7740\n",
      "Epoch 189/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 14083866.2726\n",
      "Epoch 190/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 13759701.2729\n",
      "Epoch 191/200\n",
      "457/457 [==============================] - 3s 7ms/step - loss: 13953061.7133\n",
      "Epoch 192/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 13994353.6432\n",
      "Epoch 193/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 17771057.7519\n",
      "Epoch 194/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 13875934.8684\n",
      "Epoch 195/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 14993588.3199\n",
      "Epoch 196/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 11485938.2789\n",
      "Epoch 197/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 21520549.7204\n",
      "Epoch 198/200\n",
      "457/457 [==============================] - 3s 7ms/step - loss: 7588356.5717\n",
      "Epoch 199/200\n",
      "457/457 [==============================] - 3s 7ms/step - loss: 6821980.9757\n",
      "Epoch 200/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 10967922.3318\n",
      "915.766045341147 -0.037301235516328024 0.0\n"
     ]
    }
   ],
   "source": [
    "df_forecasting = pd.read_csv('../dataset/radar/features/weather_lags5.csv')\n",
    "coef = 0.5\n",
    "X, y = df_forecasting.iloc[:,1:], df_forecasting.iloc[:,0] \n",
    "trainX, testX = X[:int(coef * len(X))].values, X[int(coef * len(X)):].values\n",
    "trainY, testY = y[:int(coef * len(y))].values, y[int(coef * len(y)):].values\n",
    "#trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "#testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "pred = LSTM_model(trainX, trainY, testX)\n",
    "mae = metrics.mean_absolute_error(testY, pred)\n",
    "r2 = metrics.r2_score(testY, pred)\n",
    "var = metrics.explained_variance_score(testY, pred)\n",
    "print(mae, r2, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481dbee9",
   "metadata": {},
   "source": [
    "### weather(t-5), intensity(t-5) selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd37e1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "457/457 [==============================] - 11s 7ms/step - loss: 10455933.2862\n",
      "Epoch 2/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 17776160.3929\n",
      "Epoch 3/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 12330964.9753\n",
      "Epoch 4/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 11597013.8796\n",
      "Epoch 5/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 14196759.9274\n",
      "Epoch 6/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 18429431.5997\n",
      "Epoch 7/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 8100074.8900\n",
      "Epoch 8/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 11364504.7168\n",
      "Epoch 9/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 10726528.4748\n",
      "Epoch 10/200\n",
      "457/457 [==============================] - 5s 10ms/step - loss: 11872321.9910\n",
      "Epoch 11/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 13714461.0579\n",
      "Epoch 12/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 13758769.0985\n",
      "Epoch 13/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 11781049.6261\n",
      "Epoch 14/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 12589105.9606\n",
      "Epoch 15/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 19724154.3537\n",
      "Epoch 16/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 14341079.3644\n",
      "Epoch 17/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 15511185.6925\n",
      "Epoch 18/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 12573913.3279\n",
      "Epoch 19/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 13853525.9479\n",
      "Epoch 20/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 14112100.2571\n",
      "Epoch 21/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 11108462.5523\n",
      "Epoch 22/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 9769023.2450\n",
      "Epoch 23/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 16953453.1301\n",
      "Epoch 24/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 13188782.7977\n",
      "Epoch 25/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 14327996.2152\n",
      "Epoch 26/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 5638734.5030\n",
      "Epoch 27/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 10571797.5071\n",
      "Epoch 28/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 9568397.1683\n",
      "Epoch 29/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 11042017.7373\n",
      "Epoch 30/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 13151494.7455\n",
      "Epoch 31/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 11087078.0089\n",
      "Epoch 32/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 19963899.9403\n",
      "Epoch 33/200\n",
      "457/457 [==============================] - 3s 7ms/step - loss: 21899111.3677\n",
      "Epoch 34/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 8181175.7884\n",
      "Epoch 35/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 25045987.5808\n",
      "Epoch 36/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 13486093.2028\n",
      "Epoch 37/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 12473549.0498\n",
      "Epoch 38/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 8478640.3825\n",
      "Epoch 39/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 14247432.4888\n",
      "Epoch 40/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 17046687.8631\n",
      "Epoch 41/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 9823388.7994\n",
      "Epoch 42/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 15614885.4061\n",
      "Epoch 43/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 18033674.9705\n",
      "Epoch 44/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 15895423.7043\n",
      "Epoch 45/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 20222918.0051\n",
      "Epoch 46/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 11101931.5128\n",
      "Epoch 47/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 15549485.4656\n",
      "Epoch 48/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 16569211.0168\n",
      "Epoch 49/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 17401238.4981\n",
      "Epoch 50/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 9779052.2885\n",
      "Epoch 51/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 16909535.2710\n",
      "Epoch 52/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 12196450.1942\n",
      "Epoch 53/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 10518217.6072\n",
      "Epoch 54/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 13973617.3832\n",
      "Epoch 55/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 16609550.7822\n",
      "Epoch 56/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 8995909.2951\n",
      "Epoch 57/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 19247394.6979\n",
      "Epoch 58/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 13943006.5394\n",
      "Epoch 59/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 12554304.3162\n",
      "Epoch 60/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 11192918.8297\n",
      "Epoch 61/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 10166952.2526\n",
      "Epoch 62/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 15177842.9861\n",
      "Epoch 63/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 11353562.2298\n",
      "Epoch 64/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 33756186.7227\n",
      "Epoch 65/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 20966240.9112\n",
      "Epoch 66/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 15757994.4414\n",
      "Epoch 67/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 12719161.0424\n",
      "Epoch 68/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 17850776.9792\n",
      "Epoch 69/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 24359166.6910\n",
      "Epoch 70/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 10060034.8836\n",
      "Epoch 71/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 11710188.2408\n",
      "Epoch 72/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 12531850.8231\n",
      "Epoch 73/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 18150235.1040\n",
      "Epoch 74/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 17474783.0714\n",
      "Epoch 75/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 14081854.1654\n",
      "Epoch 76/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 17083379.8073\n",
      "Epoch 77/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 21760684.3160\n",
      "Epoch 78/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 14017331.3586\n",
      "Epoch 79/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 10743964.2220\n",
      "Epoch 80/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 14759524.9954\n",
      "Epoch 81/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 15763047.2903\n",
      "Epoch 82/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 21584219.8976\n",
      "Epoch 83/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 9490705.2808\n",
      "Epoch 84/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 19662698.9749\n",
      "Epoch 85/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 17125164.0914\n",
      "Epoch 86/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 9096154.6752\n",
      "Epoch 87/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 9695266.1379\n",
      "Epoch 88/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 10421175.5356\n",
      "Epoch 89/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 13808032.7437\n",
      "Epoch 90/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 16871770.4913\n",
      "Epoch 91/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 13509738.4997\n",
      "Epoch 92/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "457/457 [==============================] - 3s 8ms/step - loss: 10444885.5350\n",
      "Epoch 93/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 9738208.1667\n",
      "Epoch 94/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 9327709.9630\n",
      "Epoch 95/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 25890328.5876\n",
      "Epoch 96/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 17690288.0002\n",
      "Epoch 97/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 15105032.5279\n",
      "Epoch 98/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 20594955.9633\n",
      "Epoch 99/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 8143557.2329\n",
      "Epoch 100/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 15875370.1877\n",
      "Epoch 101/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 18398874.7266\n",
      "Epoch 102/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 11056303.4285\n",
      "Epoch 103/200\n",
      "457/457 [==============================] - 4s 10ms/step - loss: 15421611.7493\n",
      "Epoch 104/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 13186991.4753\n",
      "Epoch 105/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 10561589.1641\n",
      "Epoch 106/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 9100199.7589\n",
      "Epoch 107/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 14557733.2797\n",
      "Epoch 108/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 5774573.4921\n",
      "Epoch 109/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 23835183.6065\n",
      "Epoch 110/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 9421520.8609\n",
      "Epoch 111/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 11677786.4027\n",
      "Epoch 112/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 16016386.2409\n",
      "Epoch 113/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 22128593.3641\n",
      "Epoch 114/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 20142055.9665\n",
      "Epoch 115/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 8073855.2422\n",
      "Epoch 116/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 11260087.7816\n",
      "Epoch 117/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 19777044.6130\n",
      "Epoch 118/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 19864102.6190\n",
      "Epoch 119/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 13748463.2984\n",
      "Epoch 120/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 13437524.3832\n",
      "Epoch 121/200\n",
      "457/457 [==============================] - 4s 10ms/step - loss: 13382156.9407\n",
      "Epoch 122/200\n",
      "457/457 [==============================] - 5s 10ms/step - loss: 14512654.3814\n",
      "Epoch 123/200\n",
      "457/457 [==============================] - 4s 10ms/step - loss: 13359557.9427\n",
      "Epoch 124/200\n",
      "457/457 [==============================] - 4s 10ms/step - loss: 22043118.0449\n",
      "Epoch 125/200\n",
      "457/457 [==============================] - 4s 10ms/step - loss: 12439434.7359\n",
      "Epoch 126/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 21007204.6970\n",
      "Epoch 127/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 8872722.0832\n",
      "Epoch 128/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 15327139.7475\n",
      "Epoch 129/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 12101714.7531\n",
      "Epoch 130/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 13951466.4792\n",
      "Epoch 131/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 7703957.2076\n",
      "Epoch 132/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 12128296.2140\n",
      "Epoch 133/200\n",
      "457/457 [==============================] - 4s 10ms/step - loss: 20743289.0769\n",
      "Epoch 134/200\n",
      "457/457 [==============================] - 5s 11ms/step - loss: 15396776.7559\n",
      "Epoch 135/200\n",
      "457/457 [==============================] - 5s 11ms/step - loss: 11049268.2758\n",
      "Epoch 136/200\n",
      "457/457 [==============================] - 4s 10ms/step - loss: 15738209.4761\n",
      "Epoch 137/200\n",
      "457/457 [==============================] - 4s 10ms/step - loss: 12389017.4674\n",
      "Epoch 138/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 19880941.2669\n",
      "Epoch 139/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 8292165.2815\n",
      "Epoch 140/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 17853945.6760\n",
      "Epoch 141/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 12614361.7869\n",
      "Epoch 142/200\n",
      "457/457 [==============================] - 5s 10ms/step - loss: 9120112.1760\n",
      "Epoch 143/200\n",
      "457/457 [==============================] - 5s 10ms/step - loss: 11417135.3856\n",
      "Epoch 144/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 13862438.1394\n",
      "Epoch 145/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 13711395.2273\n",
      "Epoch 146/200\n",
      "457/457 [==============================] - 4s 10ms/step - loss: 11425520.2010\n",
      "Epoch 147/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 17611967.4398\n",
      "Epoch 148/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 13879995.6178\n",
      "Epoch 149/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 15810929.6114\n",
      "Epoch 150/200\n",
      "457/457 [==============================] - 5s 11ms/step - loss: 12732529.2361\n",
      "Epoch 151/200\n",
      "457/457 [==============================] - 4s 10ms/step - loss: 11744386.6651\n",
      "Epoch 152/200\n",
      "457/457 [==============================] - 5s 11ms/step - loss: 12867361.5676\n",
      "Epoch 153/200\n",
      "457/457 [==============================] - 4s 10ms/step - loss: 8177953.7109\n",
      "Epoch 154/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 11913097.2032\n",
      "Epoch 155/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 27233092.0480\n",
      "Epoch 156/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 13333788.3414\n",
      "Epoch 157/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 8563293.0562\n",
      "Epoch 158/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 11814090.4843\n",
      "Epoch 159/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 17680235.3602\n",
      "Epoch 160/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 10615348.8577\n",
      "Epoch 161/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 7981439.3804\n",
      "Epoch 162/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 13475427.8345\n",
      "Epoch 163/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 14502904.8301\n",
      "Epoch 164/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 14395813.2542\n",
      "Epoch 165/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 7867633.9683\n",
      "Epoch 166/200\n",
      "457/457 [==============================] - 3s 8ms/step - loss: 15783358.3492\n",
      "Epoch 167/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 10833133.9671\n",
      "Epoch 168/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 12544109.3025\n",
      "Epoch 169/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 10118934.6107\n",
      "Epoch 170/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 12561980.1411\n",
      "Epoch 171/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 7003381.5604\n",
      "Epoch 172/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 17853295.3385\n",
      "Epoch 173/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 12331278.1619\n",
      "Epoch 174/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 12024226.9612\n",
      "Epoch 175/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 6466931.7213\n",
      "Epoch 176/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 5623071.1472\n",
      "Epoch 177/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 14530827.2211\n",
      "Epoch 178/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 16716619.8830\n",
      "Epoch 179/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 16608675.4573\n",
      "Epoch 180/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 6907160.2921\n",
      "Epoch 181/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 16474182.8100\n",
      "Epoch 182/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "457/457 [==============================] - 4s 8ms/step - loss: 17765832.3194\n",
      "Epoch 183/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 8906220.9216\n",
      "Epoch 184/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 20287406.7665\n",
      "Epoch 185/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 16477008.4705\n",
      "Epoch 186/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 9155086.7777\n",
      "Epoch 187/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 18259543.6566\n",
      "Epoch 188/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 14619653.3671\n",
      "Epoch 189/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 19366957.7248\n",
      "Epoch 190/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 6389107.7758\n",
      "Epoch 191/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 11183963.0837\n",
      "Epoch 192/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 18194440.8126\n",
      "Epoch 193/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 11249314.5070\n",
      "Epoch 194/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 14914396.1379\n",
      "Epoch 195/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 7652583.8879\n",
      "Epoch 196/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 21127814.1473\n",
      "Epoch 197/200\n",
      "457/457 [==============================] - 4s 8ms/step - loss: 10267265.1330\n",
      "Epoch 198/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 18165027.7770\n",
      "Epoch 199/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 17172104.9870\n",
      "Epoch 200/200\n",
      "457/457 [==============================] - 4s 9ms/step - loss: 15306793.2634\n",
      "912.0557146840987 -0.038826493632312165 1.30436106360321e-10\n"
     ]
    }
   ],
   "source": [
    "df_forecasting = pd.read_csv('../dataset/radar/features/weather_lags5_selected.csv')\n",
    "coef = 0.5\n",
    "X, y = df_forecasting.iloc[:,1:], df_forecasting.iloc[:,0] \n",
    "trainX, testX = X[:int(coef * len(X))].values, X[int(coef * len(X)):].values\n",
    "trainY, testY = y[:int(coef * len(y))].values, y[int(coef * len(y)):].values\n",
    "#trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "#testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "pred = LSTM_model(trainX, trainY, testX)\n",
    "mae = metrics.mean_absolute_error(testY, pred)\n",
    "r2 = metrics.r2_score(testY, pred)\n",
    "var = metrics.explained_variance_score(testY, pred)\n",
    "print(mae, r2, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c77b69",
   "metadata": {},
   "source": [
    "### weather(t-2), intensity(t-2), mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9274e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "419/419 [==============================] - 11s 9ms/step - loss: 14005626.4950\n",
      "Epoch 2/200\n",
      "419/419 [==============================] - 4s 10ms/step - loss: 11808142.4843\n",
      "Epoch 3/200\n",
      "419/419 [==============================] - 4s 10ms/step - loss: 26910983.2430\n",
      "Epoch 4/200\n",
      "419/419 [==============================] - 4s 10ms/step - loss: 9042475.0141\n",
      "Epoch 5/200\n",
      "419/419 [==============================] - 4s 10ms/step - loss: 6516520.9909\n",
      "Epoch 6/200\n",
      "419/419 [==============================] - 4s 9ms/step - loss: 13015238.2748\n",
      "Epoch 7/200\n",
      "419/419 [==============================] - 4s 9ms/step - loss: 19044847.8893\n",
      "Epoch 8/200\n",
      "419/419 [==============================] - 4s 8ms/step - loss: 5322467.3943\n",
      "Epoch 9/200\n",
      "419/419 [==============================] - 4s 9ms/step - loss: 23253276.4168\n",
      "Epoch 10/200\n",
      "419/419 [==============================] - 4s 9ms/step - loss: 8065265.8840\n",
      "Epoch 11/200\n",
      "419/419 [==============================] - 3s 8ms/step - loss: 9886008.8455\n",
      "Epoch 12/200\n",
      "419/419 [==============================] - 3s 7ms/step - loss: 11729308.5965\n",
      "Epoch 13/200\n",
      "419/419 [==============================] - 3s 8ms/step - loss: 10395327.0202\n",
      "Epoch 14/200\n",
      "419/419 [==============================] - 4s 9ms/step - loss: 17921121.1810\n",
      "Epoch 15/200\n",
      "419/419 [==============================] - 3s 8ms/step - loss: 12811172.3930\n",
      "Epoch 16/200\n",
      "419/419 [==============================] - 3s 8ms/step - loss: 10552964.7848\n",
      "Epoch 17/200\n",
      "419/419 [==============================] - 3s 7ms/step - loss: 15012052.7740\n",
      "Epoch 18/200\n",
      "419/419 [==============================] - 3s 7ms/step - loss: 12971097.9062\n",
      "Epoch 19/200\n",
      "419/419 [==============================] - 3s 8ms/step - loss: 10547453.2195\n",
      "Epoch 20/200\n",
      "419/419 [==============================] - 3s 8ms/step - loss: 9083740.9849\n",
      "Epoch 21/200\n",
      "419/419 [==============================] - 4s 9ms/step - loss: 17931897.9529\n",
      "Epoch 22/200\n",
      "419/419 [==============================] - 4s 9ms/step - loss: 9256510.0268\n",
      "Epoch 23/200\n",
      "419/419 [==============================] - 3s 8ms/step - loss: 8755744.4162\n",
      "Epoch 24/200\n",
      "419/419 [==============================] - 3s 8ms/step - loss: 7168471.0733\n",
      "Epoch 25/200\n",
      "419/419 [==============================] - 3s 7ms/step - loss: 21163648.5274\n",
      "Epoch 26/200\n",
      "419/419 [==============================] - 3s 7ms/step - loss: 8188749.7431\n",
      "Epoch 27/200\n",
      "419/419 [==============================] - 3s 7ms/step - loss: 9885474.9923\n",
      "Epoch 28/200\n",
      "419/419 [==============================] - 3s 8ms/step - loss: 8306458.5877\n",
      "Epoch 29/200\n",
      "419/419 [==============================] - 4s 10ms/step - loss: 18042099.1024\n",
      "Epoch 30/200\n",
      "419/419 [==============================] - 4s 8ms/step - loss: 21919139.0194\n",
      "Epoch 31/200\n",
      "419/419 [==============================] - 4s 9ms/step - loss: 13454477.1369\n",
      "Epoch 32/200\n",
      "419/419 [==============================] - 4s 8ms/step - loss: 21953975.5871\n",
      "Epoch 33/200\n",
      "419/419 [==============================] - 4s 9ms/step - loss: 8176301.5381\n",
      "Epoch 34/200\n",
      "419/419 [==============================] - 4s 10ms/step - loss: 11952527.9816\n",
      "Epoch 35/200\n",
      "419/419 [==============================] - 5s 12ms/step - loss: 11168671.3147\n",
      "Epoch 36/200\n",
      "419/419 [==============================] - 5s 11ms/step - loss: 12419142.0246\n",
      "Epoch 37/200\n",
      "419/419 [==============================] - 5s 12ms/step - loss: 15366567.9108\n",
      "Epoch 38/200\n",
      "419/419 [==============================] - 5s 12ms/step - loss: 5357341.9357\n",
      "Epoch 39/200\n",
      "419/419 [==============================] - 5s 12ms/step - loss: 15005439.6155\n",
      "Epoch 40/200\n",
      "419/419 [==============================] - 5s 11ms/step - loss: 20589269.2532\n",
      "Epoch 41/200\n",
      "419/419 [==============================] - 4s 11ms/step - loss: 15484744.7127\n",
      "Epoch 42/200\n",
      "419/419 [==============================] - 4s 11ms/step - loss: 14195773.2092\n",
      "Epoch 43/200\n",
      "419/419 [==============================] - 5s 11ms/step - loss: 20819068.9288\n",
      "Epoch 44/200\n",
      "419/419 [==============================] - 5s 11ms/step - loss: 7320733.2359\n",
      "Epoch 45/200\n",
      "419/419 [==============================] - 5s 11ms/step - loss: 17433840.8795\n",
      "Epoch 46/200\n",
      "419/419 [==============================] - 5s 12ms/step - loss: 14817990.4544\n",
      "Epoch 47/200\n",
      "419/419 [==============================] - 5s 13ms/step - loss: 16785036.9298\n",
      "Epoch 48/200\n",
      "419/419 [==============================] - 5s 12ms/step - loss: 5771624.5608\n",
      "Epoch 49/200\n",
      "419/419 [==============================] - 5s 11ms/step - loss: 14398341.5851\n",
      "Epoch 50/200\n",
      "419/419 [==============================] - 5s 12ms/step - loss: 7814784.3960\n",
      "Epoch 51/200\n",
      "419/419 [==============================] - 6s 14ms/step - loss: 10519076.1450\n",
      "Epoch 52/200\n",
      "419/419 [==============================] - 5s 11ms/step - loss: 8336167.5851\n",
      "Epoch 53/200\n",
      "419/419 [==============================] - 4s 10ms/step - loss: 9296607.1405\n",
      "Epoch 54/200\n",
      "419/419 [==============================] - 4s 9ms/step - loss: 9292010.0992\n",
      "Epoch 55/200\n",
      "419/419 [==============================] - 4s 9ms/step - loss: 12518989.6663\n",
      "Epoch 56/200\n",
      "419/419 [==============================] - 4s 10ms/step - loss: 7402018.4233\n",
      "Epoch 57/200\n",
      "419/419 [==============================] - 4s 10ms/step - loss: 11689895.1104\n",
      "Epoch 58/200\n",
      "419/419 [==============================] - 5s 11ms/step - loss: 4804985.5499\n",
      "Epoch 59/200\n",
      "419/419 [==============================] - 5s 12ms/step - loss: 10258008.9023\n",
      "Epoch 60/200\n",
      "419/419 [==============================] - 5s 12ms/step - loss: 10692341.9098\n",
      "Epoch 61/200\n",
      "419/419 [==============================] - 5s 11ms/step - loss: 3997138.7251\n",
      "Epoch 62/200\n",
      "419/419 [==============================] - 5s 13ms/step - loss: 11347813.2557\n",
      "Epoch 63/200\n",
      "419/419 [==============================] - 18s 42ms/step - loss: 15772668.2360\n",
      "Epoch 64/200\n",
      "419/419 [==============================] - 20s 49ms/step - loss: 18957718.3664\n",
      "Epoch 65/200\n",
      "419/419 [==============================] - 26s 63ms/step - loss: 16603815.7770\n",
      "Epoch 66/200\n",
      "419/419 [==============================] - 31s 75ms/step - loss: 15375956.2686\n",
      "Epoch 67/200\n",
      "419/419 [==============================] - 26s 62ms/step - loss: 10571776.9323\n",
      "Epoch 68/200\n",
      "419/419 [==============================] - 26s 62ms/step - loss: 7943978.3844\n",
      "Epoch 69/200\n",
      "419/419 [==============================] - 29s 69ms/step - loss: 10299263.4873\n",
      "Epoch 70/200\n",
      "419/419 [==============================] - 28s 68ms/step - loss: 11569467.5423\n",
      "Epoch 71/200\n",
      "419/419 [==============================] - 24s 58ms/step - loss: 14099995.4467\n",
      "Epoch 72/200\n",
      "419/419 [==============================] - 14s 33ms/step - loss: 10226507.4071\n",
      "Epoch 73/200\n",
      "207/419 [=============>................] - ETA: 10s - loss: 14848344.4192"
     ]
    }
   ],
   "source": [
    "df_forecasting = pd.read_csv('../dataset/radar/features/weather_lags2_mean.csv')\n",
    "coef = 0.5\n",
    "X, y = df_forecasting.iloc[:,1:], df_forecasting.iloc[:,0] \n",
    "trainX, testX = X[:int(coef * len(X))].values, X[int(coef * len(X)):].values\n",
    "trainY, testY = y[:int(coef * len(y))].values, y[int(coef * len(y)):].values\n",
    "#trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "#testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "pred = LSTM_model(trainX, trainY, testX)\n",
    "mae = metrics.mean_absolute_error(testY, pred)\n",
    "r2 = metrics.r2_score(testY, pred)\n",
    "var = metrics.explained_variance_score(testY, pred)\n",
    "print(mae, r2, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d08450",
   "metadata": {},
   "outputs": [],
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0c78f5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self, training_set_dir, model_save_dir, output_dir=\".\", model_file_prefix='model', \n",
    "                 training_set_id_range=(0, np.Inf), training_set_length=3, scaler = 'mm', **kwargs):\n",
    "        \"\"\"\n",
    "        :param training_set_dir: directory contains the training set files. File format: 76.csv\n",
    "        :param model_save_dir: directory to receive trained model and model weights. File format: model-76.json/model-weight-76.h5\n",
    "        :param model_file_prefix='model': file prefix for model file\n",
    "        :param training_set_range=(0, np.Inf): enterprise ids in this range (a, b) would be analyzed. PS: a must be less than b\n",
    "        :param training_set_length=3: first kth columns in training set file will be used as training set and the following one is expected value\n",
    "        :param train_test_ratio=3: the ratio of training set size to test set size when splitting input data\n",
    "        :param output_dir=\".\": output directory for prediction files\n",
    "        :param scaler: scale data set using - mm: MinMaxScaler, norm: NormalDistributionScaler\n",
    "        :param **kwargs: lstm_output_dim=4: output dimension of LSTM layer;\n",
    "                        activation_lstm='relu': activation function for LSTM layers;\n",
    "                        activation_dense='relu': activation function for Dense layer;\n",
    "                        activation_last='softmax': activation function for last layer;\n",
    "                        drop_out=0.2: fraction of input units to drop;\n",
    "                        np_epoch=25, the number of epoches to train the model. epoch is one forward pass and one backward pass of all the training examples;\n",
    "                        batch_size=100: number of samples per gradient update. The higher the batch size, the more memory space you'll need;\n",
    "                        loss='categorical_crossentropy': loss function;\n",
    "                        optimizer='rmsprop'\n",
    "        \"\"\"\n",
    "        self.training_set_dir = training_set_dir\n",
    "        self.model_save_dir = model_save_dir\n",
    "        self.model_file_prefix = model_file_prefix\n",
    "        self.training_set_id_range = training_set_id_range\n",
    "        self.training_set_length = training_set_length\n",
    "        self.output_dir = output_dir\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "        if not os.path.exists(self.model_save_dir):\n",
    "            os.makedirs(self.model_save_dir)\n",
    "        self.scaler = scaler\n",
    "        self.test_size = kwargs.get('test_size', 0.2)\n",
    "        self.lstm_output_dim = kwargs.get('lstm_output_dim', 8)\n",
    "        self.activation_lstm = kwargs.get('activation_lstm', 'relu')\n",
    "        self.activation_dense = kwargs.get('activation_dense', 'relu')\n",
    "        self.activation_last = kwargs.get('activation_last', 'softmax')    # softmax for multiple output\n",
    "        self.dense_layer = kwargs.get('dense_layer', 2)  # at least 2 layers\n",
    "        self.lstm_layer = kwargs.get('lstm_layer', 2) # at least 2 layers\n",
    "        self.drop_out = kwargs.get('drop_out', 0.2)\n",
    "        self.nb_epoch = kwargs.get('nb_epoch', 25)\n",
    "        self.batch_size = kwargs.get('batch_size', 100)\n",
    "        self.loss = kwargs.get('loss', 'categorical_crossentropy')\n",
    "        self.optimizer = kwargs.get('optimizer', 'rmsprop')\n",
    "\n",
    "\n",
    "    def NN_model_train(self, trainX, trainY, testX, testY, model_save_path):\n",
    "        \"\"\"\n",
    "        :param trainX: training data set\n",
    "        :param trainY: expect value of training data\n",
    "        :param testX: test data set\n",
    "        :param testY: expect value of test data\n",
    "        :param model_save_path: h5 file to store the trained model\n",
    "        :param override: override existing models\n",
    "        :return: model after training\n",
    "        \"\"\"\n",
    "        input_dim = trainX.shape[1]\n",
    "        output_dim = len(trainY)\n",
    "\n",
    "        # print predefined parameters of current model:\n",
    "        model = Sequential()\n",
    "        # applying a LSTM layer with x dim output and y dim input. Use dropout parameter to avoid overfit\n",
    "        model.add(LSTM(units=self.lstm_output_dim, input_dim=input_dim, activation=self.activation_lstm, return_sequences=True))\n",
    "        for i in range(self.lstm_layer-2):\n",
    "            model.add(LSTM(units=self.lstm_output_dim, activation=self.activation_lstm, return_sequences=True ))\n",
    "        # return sequences should be False to avoid dim error when concatenating with dense layer\n",
    "        model.add(LSTM(units=self.lstm_output_dim, activation=self.activation_lstm, return_sequences=True))\n",
    "        # applying a full connected NN to accept output from LSTM layer\n",
    "        for i in range(self.dense_layer-1):\n",
    "            model.add(Dense(units=self.lstm_output_dim, activation=self.activation_dense))\n",
    "            model.add(Dropout(self.drop_out))\n",
    "        model.add(Dense(units=output_dim, activation=self.activation_last))\n",
    "        # configure the learning process\n",
    "        model.compile(loss=self.loss, optimizer=self.optimizer, metrics=['accuracy'])\n",
    "        # train the model with fixed number of epoches\n",
    "        model.fit(x=trainX, y=trainY, epochs=self.nb_epoch, batch_size=self.batch_size)\n",
    "        score = model.evaluate(trainX, trainY, self.batch_size)\n",
    "        print (\"Model evaluation: {}\".format(score))\n",
    "        # store model to json file\n",
    "        save_model(model, model_save_path)\n",
    "    \n",
    "    def NN_prediction(dataset, model_save_path):\n",
    "        dataset = np.asarray(dataset)\n",
    "        if not os.path.exists(model_save_path):\n",
    "            raise ValueError(\"Lstm model not found! Train one first or check your input path: {}\".format(model_save_path))\n",
    "        model = load_model(model_save_path)\n",
    "        predict_class = model.predict_classes(dataset)\n",
    "        class_prob = model.predict_proba(dataset)\n",
    "        return predict_class, class_prob\n",
    "    \n",
    "    def model_train_predict_test(self, input_file_regx=\"^(\\d+)\\.csv\", override=False):\n",
    "        \"\"\"\n",
    "        :param override=Fasle: rerun the model prediction no matter if the expected output file exists\n",
    "        :return: model file, model weights files, prediction file, discrepancy statistic bar plot file\n",
    "        \"\"\"\n",
    "        # get training sets for lstm training\n",
    "        print (\"Scanning files within select id range ...\")\n",
    "        ids, files = get_ids_and_files_in_dir(inputdir=self.training_set_dir,\n",
    "                                                          range=self.training_set_id_range,\n",
    "                                                          input_file_regx=input_file_regx)\n",
    "        print (\"Scanning done! Selected enterprise ids are {}\".format(ids))\n",
    "        if not files:\n",
    "            raise ValueError(\"No files selected in current id range. Please check the input training set directory, \"\n",
    "                             \"input enterprise id range or file format which should be '[0-9]+.csv'\")\n",
    "\n",
    "        # get train, test, validation data\n",
    "        for id_index, id_file in enumerate(files):\n",
    "            # store prediction result to prediction directory\n",
    "            enter_file = self.training_set_dir + \"/\" + id_file\n",
    "            print (\"Processing dataset - enterprise_id is: {}\".format(ids[id_index]))\n",
    "            print (\"Reading from file {}\".format(enter_file))\n",
    "            df = pd.read_csv(enter_file)\n",
    "            df.index = range(len(df.index))\n",
    "            # retrieve training X and Y columns. First column is customer_id\n",
    "            select_col = ['customer_id']\n",
    "            select_col = np.append(select_col, ['X' + str(i) for i in range(1, 1+self.training_set_length)])\n",
    "            select_col = np.append(select_col, ['Y', 'enterprise_id'])\n",
    "            df_selected = df.ix[:, select_col]\n",
    "            # remove outlier records\n",
    "            df_selected = percentile_remove_outlier(df_selected, filter_start=1, filter_end=2+self.training_set_length)\n",
    "            # scale the train columns\n",
    "            print (\"Scaling...\")\n",
    "            if self.scaler == 'mm':\n",
    "                df_scale, minVal, maxVal = MinMaxScaler(df_selected, start_col_index=1, end_col_index=self.training_set_length+1)\n",
    "            elif self.scaler == 'norm':\n",
    "                df_scale, meanVal, stdVal = NormalDistributionScaler(df_selected, start_col_index=1, end_col_index=self.training_set_length+1)\n",
    "            else:\n",
    "                raise ValueError(\"Argument scaler must be mm or norm!\")\n",
    "            # bin date y\n",
    "            df_bin, bin_boundary = binning_date_y(df_scale, y_col=1+self.training_set_length, n_group=5)\n",
    "            print (\"Bin boundary is {}\".format(bin_boundary))\n",
    "            # get train and test dataset\n",
    "            print (\"Randomly selecting training set and test set...\")\n",
    "            all_data_x = np.asarray(df_bin.ix[:, 1:1+self.training_set_length]).reshape((len(df_bin.index), 1, self.training_set_length))\n",
    "            all_data_y = np.asarray(df_bin.ix[:, 1+self.training_set_length])\n",
    "            # convert y label to one-hot dummy label\n",
    "            y_dummy_label = np.asarray(pd.get_dummies(all_data_y))\n",
    "            # format train, test, validation data\n",
    "            sub_train, val_train, sub_test, val_test = train_test_split(all_data_x, y_dummy_label, test_size=self.test_size)\n",
    "            train_x, test_x, train_y, test_y = train_test_split(sub_train, sub_test, test_size=self.test_size)\n",
    "            # create and fit the NN model\n",
    "            model_save_path = self.model_save_dir + \"/\" + self.model_file_prefix + \"-\" + str(ids[id_index]) + \".h5\"\n",
    "            # check if model file exists\n",
    "            if not os.path.exists(model_save_path) or override:\n",
    "                self.NN_model_train(train_x, train_y, test_x, test_y, model_save_path=model_save_path)\n",
    "            # generate prediction for training\n",
    "            print (\"Predicting the output of validation set...\")\n",
    "            val_predict_class, val_predict_prob = self.NN_prediction(val_train, model_save_path=model_save_path)\n",
    "            # statistic of discrepancy between expected value and real value\n",
    "            total_sample_count = len(val_predict_class)\n",
    "            val_test_label = np.asarray([list(x).index(1) for x in val_test])\n",
    "            match_count = (np.asarray(val_predict_class) == np.asarray(val_test_label.ravel())).sum()\n",
    "            print (\"Precision using validation dataset is {}\".format(float(match_count) / total_sample_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7b4776c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(453, 5, 1) (453,)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape, trainY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "634548d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = NeuralNetwork(training_set_dir = '../dataset/radar/', model_save_dir = '../dataset/radar/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4ba2d84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(453, 5, 1) (453,)\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /Users/isabelle/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py:830 train_function  *\n        return step_function(self, iterator)\n    /Users/isabelle/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py:813 run_step  *\n        outputs = model.train_step(data)\n    /Users/isabelle/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py:770 train_step  *\n        y_pred = self(x, training=True)\n    /Users/isabelle/opt/anaconda3/lib/python3.8/site-packages/keras/engine/base_layer.py:989 __call__  *\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /Users/isabelle/opt/anaconda3/lib/python3.8/site-packages/keras/engine/input_spec.py:264 assert_input_compatibility  *\n        raise ValueError('Input ' + str(input_index) +\n\n    ValueError: Input 0 is incompatible with layer sequential_20: expected shape=(None, None, 5), found shape=(None, 5, 1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-8f409c80ea14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNN_model_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_save_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../dataset/radar/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-139-e0168920f4ba>\u001b[0m in \u001b[0;36mNN_model_train\u001b[0;34m(self, trainX, trainY, testX, testY, model_save_path)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# train the model with fixed number of epoches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Model evaluation: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1156\u001b[0m                 _r=1):\n\u001b[1;32m   1157\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 763\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    764\u001b[0m             *args, **kwds))\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3048\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3050\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3051\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3444\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3277\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3278\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3279\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3280\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3281\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /Users/isabelle/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py:830 train_function  *\n        return step_function(self, iterator)\n    /Users/isabelle/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py:813 run_step  *\n        outputs = model.train_step(data)\n    /Users/isabelle/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py:770 train_step  *\n        y_pred = self(x, training=True)\n    /Users/isabelle/opt/anaconda3/lib/python3.8/site-packages/keras/engine/base_layer.py:989 __call__  *\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /Users/isabelle/opt/anaconda3/lib/python3.8/site-packages/keras/engine/input_spec.py:264 assert_input_compatibility  *\n        raise ValueError('Input ' + str(input_index) +\n\n    ValueError: Input 0 is incompatible with layer sequential_20: expected shape=(None, None, 5), found shape=(None, 5, 1)\n"
     ]
    }
   ],
   "source": [
    "rnn.NN_model_train(trainX, trainY, testX, testY, model_save_path = '../dataset/radar/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19460237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "x=[[1],[2],[3]]#\n",
    "y=[2,4,6]#\n",
    "x = np.array(x)\n",
    "y_train = np.array(y)\n",
    "x_train = np.reshape( x, (x.shape[0], x.shape[1], 1) )#Lstm\n",
    "print(x_train.shape, y_train.shape)\n",
    "model = Sequential()\n",
    "model.add( LSTM( 100, input_shape=(x_train.shape[1], x_train.shape[2]), return_sequences=True) )\n",
    "model.add( LSTM( 20, return_sequences=False ) )\n",
    "model.add( Dropout( 0.2 ) )\n",
    "model.add( Dense( 1 ) )\n",
    "model.add( Activation( 'linear' ) )\n",
    "model.compile( loss=\"mse\", optimizer=\"rmsprop\" )\n",
    "model.fit( x_train, y_train, epochs=200, batch_size=1)#\n",
    "test = [[1.5]]\n",
    "test = np.array(test)\n",
    "test = np.reshape( test, (test.shape[0], test.shape[1], 1) )#\n",
    "res = model.predict( test )\n",
    "print( res )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "177284f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "453/453 [==============================] - 13s 6ms/step - loss: 13330565.2555\n",
      "Epoch 2/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 7065341.1818\n",
      "Epoch 3/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 10866757.0096\n",
      "Epoch 4/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 14591788.6759\n",
      "Epoch 5/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 13880054.6937\n",
      "Epoch 6/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 8540644.2635\n",
      "Epoch 7/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 16943226.5983\n",
      "Epoch 8/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 11216775.0948\n",
      "Epoch 9/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 15304053.8588\n",
      "Epoch 10/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 12953980.0325\n",
      "Epoch 11/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 17421701.7456\n",
      "Epoch 12/200\n",
      "453/453 [==============================] - 2s 6ms/step - loss: 15077650.8007\n",
      "Epoch 13/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 7712717.7514\n",
      "Epoch 14/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 11968317.5995\n",
      "Epoch 15/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 20750617.4119\n",
      "Epoch 16/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 21261704.1772\n",
      "Epoch 17/200\n",
      "453/453 [==============================] - 3s 7ms/step - loss: 15947753.8601\n",
      "Epoch 18/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 14832250.1804\n",
      "Epoch 19/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 13633294.7401\n",
      "Epoch 20/200\n",
      "453/453 [==============================] - 3s 7ms/step - loss: 16345089.4518\n",
      "Epoch 21/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 14649136.9540\n",
      "Epoch 22/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 18200241.5342\n",
      "Epoch 23/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 17593841.8777\n",
      "Epoch 24/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 17950044.4207\n",
      "Epoch 25/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 18885886.9248\n",
      "Epoch 26/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 19410020.6855\n",
      "Epoch 27/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 19079306.5107\n",
      "Epoch 28/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 12161904.6761\n",
      "Epoch 29/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 10921294.5171\n",
      "Epoch 30/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 12246044.4906\n",
      "Epoch 31/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 11293094.2230\n",
      "Epoch 32/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 13530697.8254\n",
      "Epoch 33/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 8701877.4117\n",
      "Epoch 34/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 8954023.8113\n",
      "Epoch 35/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 18281484.2843\n",
      "Epoch 36/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 11190827.1393\n",
      "Epoch 37/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 23543129.2094\n",
      "Epoch 38/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 13587985.5001\n",
      "Epoch 39/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 9072871.6928\n",
      "Epoch 40/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 11237226.0114\n",
      "Epoch 41/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 9059706.5404\n",
      "Epoch 42/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 8164035.4568\n",
      "Epoch 43/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 10900728.2269\n",
      "Epoch 44/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 18120437.7257\n",
      "Epoch 45/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 11358420.6127\n",
      "Epoch 46/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 12368834.4174\n",
      "Epoch 47/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 9481277.3953\n",
      "Epoch 48/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 12061399.2136\n",
      "Epoch 49/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 16183048.4651\n",
      "Epoch 50/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 12032624.8200\n",
      "Epoch 51/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 7080885.9945\n",
      "Epoch 52/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 17659094.3506\n",
      "Epoch 53/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 9608558.6263\n",
      "Epoch 54/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 13561496.9945\n",
      "Epoch 55/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 17419664.3034\n",
      "Epoch 56/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 17569316.9771\n",
      "Epoch 57/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 15222686.2527\n",
      "Epoch 58/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 12987823.2122\n",
      "Epoch 59/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 10118932.7674\n",
      "Epoch 60/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 11351159.6196\n",
      "Epoch 61/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 20671359.4720\n",
      "Epoch 62/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 17417544.5256\n",
      "Epoch 63/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 9468640.4346\n",
      "Epoch 64/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 11346524.3926\n",
      "Epoch 65/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 10678882.5979\n",
      "Epoch 66/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 12691560.5474\n",
      "Epoch 67/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 14256577.6870\n",
      "Epoch 68/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 7629323.1117\n",
      "Epoch 69/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 24712123.0983\n",
      "Epoch 70/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 17052126.4029\n",
      "Epoch 71/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 17836990.6092\n",
      "Epoch 72/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 12054368.8756\n",
      "Epoch 73/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 7905064.4940\n",
      "Epoch 74/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 11099584.5607\n",
      "Epoch 75/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 18887260.2833\n",
      "Epoch 76/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 14054374.6104\n",
      "Epoch 77/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 9431712.1971\n",
      "Epoch 78/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 12130641.1612\n",
      "Epoch 79/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 13446916.3632\n",
      "Epoch 80/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 9805423.9277\n",
      "Epoch 81/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 13461120.4125\n",
      "Epoch 82/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 7995857.5004\n",
      "Epoch 83/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 22984129.6406\n",
      "Epoch 84/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 9420524.8810\n",
      "Epoch 85/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 14575513.4206\n",
      "Epoch 86/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 17798553.0999\n",
      "Epoch 87/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 7579429.0089\n",
      "Epoch 88/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 10192812.0579\n",
      "Epoch 89/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 19668956.1980\n",
      "Epoch 90/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 30473657.1542\n",
      "Epoch 91/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 11697757.1033\n",
      "Epoch 92/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 13781368.4169\n",
      "Epoch 93/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 8348956.3658\n",
      "Epoch 94/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 15525970.7595\n",
      "Epoch 95/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 19215005.3319\n",
      "Epoch 96/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 23768579.1873\n",
      "Epoch 97/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 10507489.8517\n",
      "Epoch 98/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 21630486.8612\n",
      "Epoch 99/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 7724247.0690\n",
      "Epoch 100/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 11031284.7876\n",
      "Epoch 101/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 14302022.1780\n",
      "Epoch 102/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 7026919.7704\n",
      "Epoch 103/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 14059228.2810\n",
      "Epoch 104/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 20721479.3888\n",
      "Epoch 105/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 17946030.6700\n",
      "Epoch 106/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 8892856.9591\n",
      "Epoch 107/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 12695401.6701\n",
      "Epoch 108/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 21909143.4055\n",
      "Epoch 109/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 21874171.1057\n",
      "Epoch 110/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 16604461.7126\n",
      "Epoch 111/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 9697912.0900\n",
      "Epoch 112/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 13685627.9926\n",
      "Epoch 113/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 11285871.7273\n",
      "Epoch 114/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 9646353.0067\n",
      "Epoch 115/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 21061497.0073\n",
      "Epoch 116/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 17570485.5022\n",
      "Epoch 117/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 13749572.9945\n",
      "Epoch 118/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 19034664.3171\n",
      "Epoch 119/200\n",
      "453/453 [==============================] - 2s 5ms/step - loss: 18471214.6372\n",
      "Epoch 120/200\n",
      "453/453 [==============================] - 2s 6ms/step - loss: 8494569.1850\n",
      "Epoch 121/200\n",
      "453/453 [==============================] - 3s 7ms/step - loss: 15074277.5632\n",
      "Epoch 122/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 12637232.6920\n",
      "Epoch 123/200\n",
      "453/453 [==============================] - 3s 7ms/step - loss: 19334780.8788\n",
      "Epoch 124/200\n",
      "453/453 [==============================] - 3s 7ms/step - loss: 11747278.5787\n",
      "Epoch 125/200\n",
      "453/453 [==============================] - 3s 7ms/step - loss: 17613291.1925\n",
      "Epoch 126/200\n",
      "453/453 [==============================] - 3s 7ms/step - loss: 12509131.2295\n",
      "Epoch 127/200\n",
      "453/453 [==============================] - 3s 7ms/step - loss: 11370760.0023\n",
      "Epoch 128/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 19116316.5083\n",
      "Epoch 129/200\n",
      "453/453 [==============================] - 4s 8ms/step - loss: 12755299.3043\n",
      "Epoch 130/200\n",
      "453/453 [==============================] - 4s 8ms/step - loss: 16308793.3714\n",
      "Epoch 131/200\n",
      "453/453 [==============================] - 4s 8ms/step - loss: 17167667.0253\n",
      "Epoch 132/200\n",
      "453/453 [==============================] - 3s 7ms/step - loss: 15772935.2588\n",
      "Epoch 133/200\n",
      "453/453 [==============================] - 3s 8ms/step - loss: 13898659.5708\n",
      "Epoch 134/200\n",
      "453/453 [==============================] - 3s 7ms/step - loss: 8499190.5406\n",
      "Epoch 135/200\n",
      "453/453 [==============================] - 3s 7ms/step - loss: 7733882.4585\n",
      "Epoch 136/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 15608311.5088\n",
      "Epoch 137/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 7186691.0216\n",
      "Epoch 138/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 23206163.1454\n",
      "Epoch 139/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 19862613.9369\n",
      "Epoch 140/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 11214720.8611\n",
      "Epoch 141/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 16248006.0939\n",
      "Epoch 142/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 12234148.4325\n",
      "Epoch 143/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 9059084.7133\n",
      "Epoch 144/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 21233171.0780\n",
      "Epoch 145/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 10426706.1814\n",
      "Epoch 146/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 13507820.0853\n",
      "Epoch 147/200\n",
      "453/453 [==============================] - 3s 7ms/step - loss: 18551930.9112\n",
      "Epoch 148/200\n",
      "453/453 [==============================] - 3s 7ms/step - loss: 11404520.9431\n",
      "Epoch 149/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 20703396.5959\n",
      "Epoch 150/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 14725481.9857\n",
      "Epoch 151/200\n",
      "453/453 [==============================] - 3s 7ms/step - loss: 12734208.0852\n",
      "Epoch 152/200\n",
      "453/453 [==============================] - 3s 7ms/step - loss: 6198335.9702\n",
      "Epoch 153/200\n",
      "453/453 [==============================] - 3s 7ms/step - loss: 15469831.1102\n",
      "Epoch 154/200\n",
      "453/453 [==============================] - 3s 7ms/step - loss: 11176840.8820\n",
      "Epoch 155/200\n",
      "453/453 [==============================] - 3s 7ms/step - loss: 9529722.8338\n",
      "Epoch 156/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 12650094.8755\n",
      "Epoch 157/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 21403535.8704\n",
      "Epoch 158/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 15098966.6961\n",
      "Epoch 159/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 14111548.5401\n",
      "Epoch 160/200\n",
      "453/453 [==============================] - 3s 7ms/step - loss: 16038357.0696\n",
      "Epoch 161/200\n",
      "453/453 [==============================] - 3s 7ms/step - loss: 17210657.6626\n",
      "Epoch 162/200\n",
      "453/453 [==============================] - 3s 7ms/step - loss: 10894996.6631\n",
      "Epoch 163/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 20509680.3804\n",
      "Epoch 164/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 21628285.2624\n",
      "Epoch 165/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 9058794.7291\n",
      "Epoch 166/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 14286103.1764\n",
      "Epoch 167/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 15032342.2884\n",
      "Epoch 168/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 5249302.5648\n",
      "Epoch 169/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 9344899.1353\n",
      "Epoch 170/200\n",
      "453/453 [==============================] - 3s 7ms/step - loss: 12145076.7528\n",
      "Epoch 171/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 10989441.6849\n",
      "Epoch 172/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 23995115.5226\n",
      "Epoch 173/200\n",
      "453/453 [==============================] - 3s 7ms/step - loss: 14983906.4295\n",
      "Epoch 174/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 10274044.5438\n",
      "Epoch 175/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 14583227.1698\n",
      "Epoch 176/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 16403734.8844\n",
      "Epoch 177/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 12520754.8634\n",
      "Epoch 178/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 12631569.8608\n",
      "Epoch 179/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 14576651.1589\n",
      "Epoch 180/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 10255470.0796\n",
      "Epoch 181/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 8321479.9426\n",
      "Epoch 182/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453/453 [==============================] - 3s 6ms/step - loss: 9170331.1077\n",
      "Epoch 183/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 10003452.7933\n",
      "Epoch 184/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 16805518.1138\n",
      "Epoch 185/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 13169382.2814\n",
      "Epoch 186/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 15990108.7515\n",
      "Epoch 187/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 9630123.5632\n",
      "Epoch 188/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 21179460.0682\n",
      "Epoch 189/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 18537666.8852\n",
      "Epoch 190/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 17175780.4532\n",
      "Epoch 191/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 13928471.6193\n",
      "Epoch 192/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 16143660.2240\n",
      "Epoch 193/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 14231668.4910\n",
      "Epoch 194/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 9718588.9303\n",
      "Epoch 195/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 14364403.2724\n",
      "Epoch 196/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 14194349.1436\n",
      "Epoch 197/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 16366936.3596\n",
      "Epoch 198/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 9785578.8136\n",
      "Epoch 199/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 12249245.1808\n",
      "Epoch 200/200\n",
      "453/453 [==============================] - 3s 6ms/step - loss: 11931573.2676\n"
     ]
    }
   ],
   "source": [
    "pred = LSTM_model(trainX, trainY, testX)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
